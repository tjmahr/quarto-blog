[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TJ Mahr’s blog",
    "section": "",
    "text": "Builder Teej is yelling something at us! 👷\n\n\n\nWhoa whoa, you see the barricades here? 🚧🚧🚧\nThis place is under construction! This is just a temporary sandbox while I (figure out how to) migrate to a quarto blog!\nAnd besides I can’t let you in without a hard hat on.\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOrdering constraints in brms using contrast coding\n\n\n\n\n\nBut mostly how contrast matrices are computed\n\n\n\n\n\n\nJul 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nplaying with quatro\n\n\n\n\n\nthis is STILL in beta\n\n\n\n\n\n\nJul 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSelf-documenting plots in ggplot2\n\n\n\n\n\nIncluding plotting code as an annotation on a plot\n\n\n\n\n\n\nMar 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLists are my secret weapon for reporting stats with knitr\n\n\n\n\n\nTidying and splitting model summaries for inline reporting\n\n\n\n\n\n\nFeb 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nset_na_where(): a nonstandard evaluation use case\n\n\n\n\n\nBottling up magic spells\n\n\n\n\n\n\nAug 15, 2017\n\n\n\n\n\n\n  \n\n\n\n\nRecent adventures with lazyeval\n\n\n\n\n\nSome basic uses of nonstandard evaluation.\n\n\n\n\n\n\nAug 15, 2016\n\n\n\n\n\n\n  \n\n\n\n\nConfusion matrix statistics on late talker diagnoses\n\n\n\n\n\nPosterior predictive values and the like.\n\n\n\n\n\n\nOct 6, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2015-10-06-confusion-matrix-late-talkers/index.html",
    "href": "posts/2015-10-06-confusion-matrix-late-talkers/index.html",
    "title": "Confusion matrix statistics on late talker diagnoses",
    "section": "",
    "text": "How many late talkers are just late bloomers? More precisely, how many children identified as late talkers at 18 months catch up to the normal range by one year later? This is an important question. From a clinical perspective, we want to support children with language delays, but it is also inefficient to spend resources fixing a self-correcting problem.\nFernald and Marchman (2012) touch on this question. Children falling below the 20th percentile in vocabulary score at 18 months were labeled “late talkers”. These children, along with a control group of timely-talkers, participated in an eyetracking study at 18 months and had their vocabulary measured every 3 months until 30 months of age.\nIn their sample, 22 of 36 late talkers were late bloomers, catching up to the normal vocabulary range at 30 months, and 42 of 46 timely talkers remained in the normal range of vocab development. The authors later report that eyetracking reaction times at 18 months predicted rates of vocabulary growth in both groups. In particular, the late-bloomers were significantly faster than the children who did not catch up.\nThe authors repeatedly report confusion matrix statistics on different subsets of the data. Which make sense: The question of late bloomers is also a question about the positive predictive value of a late-talker diagnosis. In the majority of cases, a “late talker” label at 18 months did not predict continued delay one year later. Therefore, the diagnosis has poor positive predictive value (14/36 = 39%)."
  },
  {
    "objectID": "posts/2015-10-06-confusion-matrix-late-talkers/index.html#confusion-matrix-measures-in-r",
    "href": "posts/2015-10-06-confusion-matrix-late-talkers/index.html#confusion-matrix-measures-in-r",
    "title": "Confusion matrix statistics on late talker diagnoses",
    "section": "Confusion Matrix Measures in R",
    "text": "Confusion Matrix Measures in R\nI would like to report similar classification quantities in my own analyses, so I figured out how to reproduce their results in R. And it’s as simple as calling the caret::confusionMatrix() function in the caret package.\nFirst, let’s re-create their data. We’ll make a long dataframe with one row per child reported in the study. We will have fields for each child’s initial Group (late talking or within-normal-limits at 18 months), their Predicted group (assuming late-talking children remain delayed), and the observed Outcome.\n\nlibrary(dplyr)\n\n# LT: late talking\n# WNL: within normal limits\ngroups &lt;- c(\"WNL at 18m\", \"LT at 18m\")\noutcomes &lt;- c(\"WNL at 30m\", \"Delayed at 30m\")\n\n# Counts from paper\nlt_still_delayed &lt;- 14\nlt_bloomed &lt;- 22\n\nwnl_still_wnl &lt;- 42\nwnl_delayed &lt;- 4\n\n# Reproduce their data-set (one row per reported child)\nwnl_data &lt;- tibble(\n  Group = groups[1],\n  Predicted = outcomes[1],\n  Outcome = rep(outcomes, times = c(wnl_still_wnl, wnl_delayed))\n)\n\nlt_data &lt;- tibble(\n  Group = \"LT at 18m\",\n  Outcome = rep(outcomes, times = c(lt_bloomed, lt_still_delayed)),\n  Predicted = outcomes[2]\n)\n\nall_kids &lt;- bind_rows(wnl_data, lt_data) %&gt;%\n  mutate(ChildID = seq_along(Outcome)) %&gt;% \n  select(ChildID, Group, Predicted, Outcome) %&gt;% \n  mutate(\n    Predicted = factor(Predicted, outcomes),\n    Outcome = factor(Outcome, outcomes)\n  )\n\nWhat we have looks like a real data-set now.\n\n\n\n\nall_kids %&gt;% \n  sample_n(8, replace = FALSE) %&gt;% \n  arrange(Group, Predicted, Outcome)\n#&gt; # A tibble: 8 × 4\n#&gt;   ChildID Group      Predicted      Outcome   \n#&gt;     &lt;int&gt; &lt;chr&gt;      &lt;fct&gt;          &lt;fct&gt;     \n#&gt; 1      47 LT at 18m  Delayed at 30m WNL at 30m\n#&gt; 2      52 LT at 18m  Delayed at 30m WNL at 30m\n#&gt; 3      60 LT at 18m  Delayed at 30m WNL at 30m\n#&gt; 4       1 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 5      16 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 6      19 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 7      34 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 8      27 WNL at 18m WNL at 30m     WNL at 30m\n\nNext, we just call caret::confusionMatrix() on the predicted values and the reference values.\n\nconf_mat &lt;- caret::confusionMatrix(all_kids$Predicted, all_kids$Outcome)\nconf_mat\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;                 Reference\n#&gt; Prediction       WNL at 30m Delayed at 30m\n#&gt;   WNL at 30m             42              4\n#&gt;   Delayed at 30m         22             14\n#&gt;                                           \n#&gt;                Accuracy : 0.6829          \n#&gt;                  95% CI : (0.5708, 0.7813)\n#&gt;     No Information Rate : 0.7805          \n#&gt;     P-Value [Acc &gt; NIR] : 0.9855735       \n#&gt;                                           \n#&gt;                   Kappa : 0.3193          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 0.0008561       \n#&gt;                                           \n#&gt;             Sensitivity : 0.6562          \n#&gt;             Specificity : 0.7778          \n#&gt;          Pos Pred Value : 0.9130          \n#&gt;          Neg Pred Value : 0.3889          \n#&gt;              Prevalence : 0.7805          \n#&gt;          Detection Rate : 0.5122          \n#&gt;    Detection Prevalence : 0.5610          \n#&gt;       Balanced Accuracy : 0.7170          \n#&gt;                                           \n#&gt;        'Positive' Class : WNL at 30m      \n#&gt; \n\n\n\n\nHere, we can confirm the positive predictive value (true positives / positive calls)1 is 14/36 = 0.913. The negative predictive value is noteworthy; most children not diagnosed as late talkers did not show a delay one year later (NPV = 42/46 = 0.3889).\n\n\n\n\n\n\n\nSession info\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21 ucrt)\n os       Windows 11 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-28\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n caret          6.0-94     2023-03-21 [1] CRAN (R 4.3.1)\n class          7.3-21     2023-01-23 [2] CRAN (R 4.3.0)\n cli            3.6.1      2023-03-23 [1] CRAN (R 4.3.0)\n codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.0)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.0)\n data.table     1.14.8     2023-02-17 [1] CRAN (R 4.3.0)\n digest         0.6.33     2023-07-07 [1] CRAN (R 4.3.1)\n dplyr        * 1.1.2      2023-04-20 [1] CRAN (R 4.3.0)\n e1071          1.7-13     2023-02-01 [1] CRAN (R 4.3.1)\n evaluate       0.21       2023-05-05 [1] CRAN (R 4.3.0)\n fansi          1.0.4      2023-01-22 [1] CRAN (R 4.3.0)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.1)\n future         1.33.0     2023-07-01 [1] CRAN (R 4.3.0)\n future.apply   1.11.0     2023-05-21 [1] CRAN (R 4.3.1)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2        3.4.2      2023-04-03 [1] CRAN (R 4.3.0)\n globals        0.16.2     2022-11-21 [1] CRAN (R 4.3.0)\n glue           1.6.2      2022-02-24 [1] CRAN (R 4.3.0)\n gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.0)\n gtable         0.3.3      2023-03-21 [1] CRAN (R 4.3.0)\n hardhat        1.3.0      2023-03-30 [1] CRAN (R 4.3.1)\n htmltools      0.5.5      2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets    1.6.2      2023-03-17 [1] CRAN (R 4.3.0)\n ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.1)\n iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7      2023-06-29 [1] CRAN (R 4.3.1)\n knitr          1.43       2023-05-25 [1] CRAN (R 4.3.0)\n lattice        0.21-8     2023-04-05 [2] CRAN (R 4.3.0)\n lava           1.7.2.1    2023-02-27 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3      2022-10-07 [1] CRAN (R 4.3.0)\n listenv        0.9.0      2022-12-16 [1] CRAN (R 4.3.0)\n lubridate      1.9.2      2023-02-10 [1] CRAN (R 4.3.0)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n MASS           7.3-60     2023-05-04 [1] CRAN (R 4.3.0)\n Matrix         1.6-0      2023-07-08 [1] CRAN (R 4.3.1)\n ModelMetrics   1.2.2.2    2020-03-17 [1] CRAN (R 4.3.1)\n munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n nlme           3.1-162    2023-01-31 [2] CRAN (R 4.3.0)\n nnet           7.3-19     2023-05-03 [1] CRAN (R 4.3.0)\n parallelly     1.36.0     2023-05-26 [1] CRAN (R 4.3.0)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n plyr           1.8.8      2022-11-11 [1] CRAN (R 4.3.0)\n pROC           1.18.4     2023-07-06 [1] CRAN (R 4.3.1)\n prodlim        2023.03.31 2023-04-02 [1] CRAN (R 4.3.1)\n proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.1)\n purrr          1.0.1      2023-01-10 [1] CRAN (R 4.3.0)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n ragg           1.2.5      2023-01-12 [1] CRAN (R 4.3.0)\n Rcpp           1.0.11     2023-07-06 [1] CRAN (R 4.3.1)\n recipes        1.0.6      2023-04-25 [1] CRAN (R 4.3.1)\n reshape2       1.4.4      2020-04-09 [1] CRAN (R 4.3.0)\n rlang          1.1.1      2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown      2.23       2023-07-01 [1] CRAN (R 4.3.0)\n rpart          4.1.19     2022-10-21 [2] CRAN (R 4.3.0)\n rstudioapi     0.15.0     2023-07-07 [1] CRAN (R 4.3.1)\n scales         1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringi        1.7.12     2023-01-11 [1] CRAN (R 4.3.0)\n stringr        1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n survival       3.5-5      2023-03-12 [2] CRAN (R 4.3.0)\n systemfonts    1.0.4      2022-02-11 [1] CRAN (R 4.3.0)\n textshaping    0.3.6      2021-10-13 [1] CRAN (R 4.3.0)\n tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n timechange     0.2.0      2023-01-11 [1] CRAN (R 4.3.0)\n timeDate       4022.108   2023-01-07 [1] CRAN (R 4.3.0)\n utf8           1.2.3      2023-01-31 [1] CRAN (R 4.3.0)\n vctrs          0.6.3      2023-06-14 [1] CRAN (R 4.3.1)\n withr          2.5.0      2022-03-03 [1] CRAN (R 4.3.0)\n xfun           0.39       2023-04-20 [1] CRAN (R 4.3.0)\n yaml           2.3.7      2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.0/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-10-06-confusion-matrix-late-talkers/index.html#footnotes",
    "href": "posts/2015-10-06-confusion-matrix-late-talkers/index.html#footnotes",
    "title": "Confusion matrix statistics on late talker diagnoses",
    "section": "Footnotes",
    "text": "Footnotes\n\nTechnically, caret uses the sensitivity, specificity and prevalence form of the PPV calculation.↩︎"
  },
  {
    "objectID": "posts/2016-08-15-recent-adventures-with-lazyeval/index.html",
    "href": "posts/2016-08-15-recent-adventures-with-lazyeval/index.html",
    "title": "Recent adventures with lazyeval",
    "section": "",
    "text": "The lazyeval package is a toolset for performing nonstandard evaluation in R. Nonstandard evaluation refers to any situation where something special happens with how user input or code is evaluated.\nFor example, the library() function doesn’t evaluate variables. In the example below, I try to trick library() into loading a fake package called evil_package by assigning to the package name lazyeval. In other words, we have the expression lazyeval and its value is \"evil_package\".\nprint(.packages())\n#&gt; [1] \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"  \n#&gt; [7] \"base\"\n\nlazyeval &lt;- \"evil_package\"\nlibrary(lazyeval)\n\n# The lazyeval package is loaded now.\nprint(.packages())\n#&gt; [1] \"lazyeval\"  \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n#&gt; [7] \"methods\"   \"base\"\nBut this gambit doesn’t work because library() did something special: It didn’t evaluate the expression lazyeval. In the source code of library(), there is a line package &lt;- as.character(substitute(package)) which replaces the value of package with a character version of the expression that the user wrote.\nThat’s a simple example of nonstandard evaluation, but it’s pervasive. It’s why you never have to quote column names in dplyr or ggplot2. In this post, I present some recent examples where I decided to use the lazyeval package to do something nonstandard. These examples are straight out of the lazyeval vignette in terms of complexity, but that’s fine. We all have to start somewhere."
  },
  {
    "objectID": "posts/2016-08-15-recent-adventures-with-lazyeval/index.html#capturing-expressions",
    "href": "posts/2016-08-15-recent-adventures-with-lazyeval/index.html#capturing-expressions",
    "title": "Recent adventures with lazyeval",
    "section": "Capturing expressions",
    "text": "Capturing expressions\nPlot titles. While reading the book Machine Learning For Hackers, I wanted to plot random numbers generated by probability distributions discussed by the book. I used the lazyeval::expr_text() function to capture the command used to generate the numbers and write it as the title of the plot.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\nplot_dist &lt;- function(xs) {\n  data &lt;- tibble(x = xs)\n  ggplot(data) +\n    aes(x = x) +\n    geom_density() +\n    ggtitle(lazyeval::expr_text(xs)) \n}\n\nplot_dist(rcauchy(n = 250, location = 0, scale = 1))\n\n\n\nExamples of plot_dist()\n\n\nplot_dist(rgamma(n = 25000, shape = 5, rate = .5))\n\n\n\nExamples of plot_dist()\n\n\nplot_dist(rexp(n = 25000, rate = .5))\n\n\n\nExamples of plot_dist()\n\n\n\nLess fussy warning messages. I recently inherited some code where there were custom warning messages based on the input. The code threw a warning whenever a duplicate participant ID was found in a survey. It went something like this:\n\n# some dummy data\nstudy1 &lt;- tibble(\n  id = c(1, 2, 3, 4), \n  response = c(\"b\", \"c\", \"a\", \"b\")\n)\n\nstudy2 &lt;- tibble(\n  id = c(1, 2, 2, 3, 1), \n  response = c(\"a\", \"a\", \"a\", \"b\", \"c\")\n)\n\nif (anyDuplicated(study1$id)) {\n  warning(\"Duplicate IDs found in Study1\", call. = FALSE)\n}\n\nif (anyDuplicated(study2$id)) {\n  warning(\"Duplicate IDs found in Study2\", call. = FALSE)\n}\n#&gt; Warning: Duplicate IDs found in Study2\n\nTo extend this code to a new study, one would just copy-and-paste and update the if statement’s condition and warning messages. Like so:\n\nstudy3 &lt;- tibble(\n  id = c(1, 2, 3, 2), \n  response = c(\"b\", \"c\", \"a\", \"b\")\n)\n\nif (anyDuplicated(study3$id)) {\n  warning(\"Duplicate IDs found in Study2\", call. = FALSE)\n}\n#&gt; Warning: Duplicate IDs found in Study2\n\nWait, that’s not right! I forgot to update the warning message…\nThis setup is too brittle for me, so I abstracted the procedure into a function. First, I wrote a helper function to print out duplicates elements in a vector. This helper will let us make the warning message a little more informative.\n\n# Print out duplicated elements in a vector\nprint_duplicates &lt;- function(xs) {\n  duplicated &lt;- xs[duplicated(xs)]\n  duplicated %&gt;% sort() %&gt;% unique() %&gt;% paste0(collapse = \", \")\n}\n\nprint_duplicates(study2$id)\n#&gt; [1] \"1, 2\"\n\nNext, I wrote a function to issue the warnings. I used lazyeval::expr_label() convert the user-inputted expression into a string wrapped in backticks.\n\n# Print a warning if duplicates are found in a vector\nwarn_duplicates &lt;- function(xs) {\n  if (anyDuplicated(xs)) {\n    # Get what the user wrote for the xs argument\n    actual_xs &lt;- lazyeval::expr_label(xs)\n    msg &lt;- paste0(\n      \"Duplicate entries in \", actual_xs, \": \", print_duplicates(xs)\n    )\n    warning(msg, call. = FALSE)\n  }\n  invisible(NULL)\n}\n\nwarn_duplicates(study1$id)\nwarn_duplicates(study2$id)\n#&gt; Warning: Duplicate entries in `study2$id`: 1, 2\nwarn_duplicates(study3$id)\n#&gt; Warning: Duplicate entries in `study3$id`: 2\n\nThe advantage of this approach is that the warning is a generic message that can work on any input. But in a funny way, the warning is also customized for the input because it includes the input printed verbatim.\nAn aside: In plotting, I used lazyeval::expr_text(), but here I used lazyeval::expr_label(). The two differ slightly. Namely, expr_label() surrounds the captured expression with backticks to indicate that expression is code:\n\nlazyeval::expr_text(stop())\n#&gt; [1] \"stop()\"\nlazyeval::expr_label(stop())\n#&gt; [1] \"`stop()`\"\n\n# 2021-01-05: rlang requires you to separate the quoting from the quoting\nrlang::quo_text(quote(stop()))\n#&gt; [1] \"stop()\"\nrlang::quo_label(quote(stop()))\n#&gt; [1] \"`stop()`\"\n\nrlang::quo_text(rlang::quo(stop()))\n#&gt; [1] \"stop()\"\nrlang::quo_label(rlang::quo(stop()))\n#&gt; [1] \"`stop()`\""
  },
  {
    "objectID": "posts/2016-08-15-recent-adventures-with-lazyeval/index.html#asking-questions-about-a-posterior-distribution",
    "href": "posts/2016-08-15-recent-adventures-with-lazyeval/index.html#asking-questions-about-a-posterior-distribution",
    "title": "Recent adventures with lazyeval",
    "section": "Asking questions about a posterior distribution",
    "text": "Asking questions about a posterior distribution\nI fit regression models with RStanARM. It lets me fit Bayesian models in Stan by writing conventional R modeling code. (Btw, I’m giving a tutorial on RStanARM in a month.)\nHere’s a model about some famous flowers.\n\nlibrary(rstanarm)\n#&gt; Loading required package: Rcpp\n#&gt; Warning: package 'Rcpp' was built under R version 4.3.1\n#&gt; This is rstanarm version 2.21.4\n#&gt; - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n#&gt; - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n#&gt; - For execution on a local, multicore CPU with excess RAM we recommend calling\n#&gt;   options(mc.cores = parallel::detectCores())\n\nmodel &lt;- stan_glm(\n  Petal.Width ~ Petal.Length * Species,\n  data = iris,\n  family = gaussian(), \n  prior = normal(0, 1),\n  seed = \"20230718\"\n)\n\nHere’s the essential relationship being explored.\n\nggplot(iris) + \n  aes(x = Petal.Length, y = Petal.Width, color = Species) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nThe model gives me 4000 samples from the posterior distribution of the model.\n\nsummary(model)\n#&gt; \n#&gt; Model Info:\n#&gt;  function:     stan_glm\n#&gt;  family:       gaussian [identity]\n#&gt;  formula:      Petal.Width ~ Petal.Length * Species\n#&gt;  algorithm:    sampling\n#&gt;  sample:       4000 (posterior sample size)\n#&gt;  priors:       see help('prior_summary')\n#&gt;  observations: 150\n#&gt;  predictors:   6\n#&gt; \n#&gt; Estimates:\n#&gt;                                  mean   sd   10%   50%   90%\n#&gt; (Intercept)                     0.0    0.2 -0.3   0.0   0.3 \n#&gt; Petal.Length                    0.2    0.1  0.0   0.2   0.3 \n#&gt; Speciesversicolor              -0.1    0.3 -0.5  -0.1   0.3 \n#&gt; Speciesvirginica                1.1    0.3  0.7   1.1   1.5 \n#&gt; Petal.Length:Speciesversicolor  0.2    0.1  0.0   0.2   0.4 \n#&gt; Petal.Length:Speciesvirginica   0.0    0.1 -0.2   0.0   0.2 \n#&gt; sigma                           0.2    0.0  0.2   0.2   0.2 \n#&gt; \n#&gt; Fit Diagnostics:\n#&gt;            mean   sd   10%   50%   90%\n#&gt; mean_PPD 1.2    0.0  1.2   1.2   1.2  \n#&gt; \n#&gt; The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n#&gt; \n#&gt; MCMC diagnostics\n#&gt;                                mcse Rhat n_eff\n#&gt; (Intercept)                    0.0  1.0   858 \n#&gt; Petal.Length                   0.0  1.0   862 \n#&gt; Speciesversicolor              0.0  1.0  1147 \n#&gt; Speciesvirginica               0.0  1.0  1165 \n#&gt; Petal.Length:Speciesversicolor 0.0  1.0   839 \n#&gt; Petal.Length:Speciesvirginica  0.0  1.0   829 \n#&gt; sigma                          0.0  1.0  2756 \n#&gt; mean_PPD                       0.0  1.0  3166 \n#&gt; log-posterior                  0.0  1.0  1394 \n#&gt; \n#&gt; For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nAt the 2.5% quantile, the Petal.Length effect looks like zero or less than zero. What proportion of the Petal.Length effects (for setosa flowers) is positive?\nTo answer questions like this one in a convenient way, I wrote a function that takes a boolean expression about a model’s parameters and evaluates it inside of the data-frame summary of the model posterior distribution. lazyeval::f_eval() does the nonstandard evaluation: It evaluates an expression captured by a formula like ~ 0 &lt; Petal.Length inside of a list or data-frame. (Note that the mean of a logical vector is the proportion of the elements that are true.)\n\n# Get proportion of posterior samples satisfying some inequality\nposterior_proportion_ &lt;- function(model, inequality) {\n  draws &lt;- as.data.frame(model)\n  mean(lazyeval::f_eval(inequality, data = draws))\n}\n\nposterior_proportion_(model, ~ 0 &lt; Petal.Length)\n#&gt; [1] 0.87675\n\nBut all those tildes… The final underscore in posterior_proportion_() follows a convention for distinguishing between nonstandard evaluation functions that require formulas and those that do not. In the dplyr package, for example, there is select_()/select()/, mutate_()/mutate(), and so on. We can do the formula-free form of this function by using lazyeval::f_capture() to capture the input expression as a formula. We then hand that off to the version of the function that takes a formula.\n\nposterior_proportion &lt;- function(model, expr) {\n  posterior_proportion_(model, lazyeval::f_capture(expr))\n}\n\nposterior_proportion(model, 0 &lt; Petal.Length)\n#&gt; [1] 0.87675\n\nHere’s another question: What proportion of the posterior of the Petal.Length effect for virginica flowers is positive? In classical models, we would change the reference level for the categorical variable, refit the model, and check the significance. But I don’t want to refit this model because that would repeat the MCMC sampling. (It takes about 30 seconds to fit this model after all!) I’ll just ask the model for the sum of Petal.Length and Petal.Length:Speciesversicolor effects. That will give me the estimated Petal.Length effect but adjusted for the versicolor species.\n\nposterior_proportion(model, 0 &lt; Petal.Length + `Petal.Length:Speciesversicolor`)\n#&gt; [1] 1\n\nposterior_proportion(model, 0 &lt; Petal.Length + `Petal.Length:Speciesvirginica`)\n#&gt; [1] 1\n\n(The backticks around Petal.Length:Speciesversicolor here prevent the : symbol from being evaluated as an operator.)\n\n\n\n\n\n\n\nSession info\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting      value\n version      R version 4.3.0 (2023-04-21 ucrt)\n os           Windows 11 x64 (build 22621)\n system       x86_64, mingw32\n ui           RTerm\n language     (EN)\n collate      English_United States.utf8\n ctype        English_United States.utf8\n tz           America/Chicago\n date         2023-07-28\n pandoc       3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n stan (rstan) 2.26.1\n quarto       1.3.353\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version  date (UTC) lib source\n   base64enc      0.1-3    2015-07-28 [1] CRAN (R 4.3.0)\n   bayesplot      1.10.0   2022-11-16 [1] CRAN (R 4.3.0)\n   boot           1.3-28.1 2022-11-22 [2] CRAN (R 4.3.0)\n   callr          3.7.3    2022-11-02 [1] CRAN (R 4.3.0)\n   cli            3.6.1    2023-03-23 [1] CRAN (R 4.3.0)\n   codetools      0.2-19   2023-02-01 [2] CRAN (R 4.3.0)\n   colorspace     2.1-0    2023-01-23 [1] CRAN (R 4.3.0)\n   colourpicker   1.2.0    2022-10-28 [1] CRAN (R 4.3.0)\n   crayon         1.5.2    2022-09-29 [1] CRAN (R 4.3.0)\n   crosstalk      1.2.0    2021-11-04 [1] CRAN (R 4.3.0)\n   curl           5.0.1    2023-06-07 [1] CRAN (R 4.3.0)\n   digest         0.6.33   2023-07-07 [1] CRAN (R 4.3.1)\n   dplyr        * 1.1.2    2023-04-20 [1] CRAN (R 4.3.0)\n   DT             0.28     2023-05-18 [1] CRAN (R 4.3.0)\n   dygraphs       1.1.1.6  2018-07-11 [1] CRAN (R 4.3.0)\n   ellipsis       0.3.2    2021-04-29 [1] CRAN (R 4.3.0)\n   evaluate       0.21     2023-05-05 [1] CRAN (R 4.3.0)\n   fansi          1.0.4    2023-01-22 [1] CRAN (R 4.3.0)\n   farver         2.1.1    2022-07-06 [1] CRAN (R 4.3.0)\n   fastmap        1.1.1    2023-02-24 [1] CRAN (R 4.3.0)\n   generics       0.1.3    2022-07-05 [1] CRAN (R 4.3.0)\n   ggplot2      * 3.4.2    2023-04-03 [1] CRAN (R 4.3.0)\n   glue           1.6.2    2022-02-24 [1] CRAN (R 4.3.0)\n   gridExtra      2.3      2017-09-09 [1] CRAN (R 4.3.0)\n   gtable         0.3.3    2023-03-21 [1] CRAN (R 4.3.0)\n   gtools         3.9.4    2022-11-27 [1] CRAN (R 4.3.0)\n   htmltools      0.5.5    2023-03-23 [1] CRAN (R 4.3.0)\n   htmlwidgets    1.6.2    2023-03-17 [1] CRAN (R 4.3.0)\n   httpuv         1.6.11   2023-05-11 [1] CRAN (R 4.3.0)\n   igraph         1.5.0.1  2023-07-23 [1] CRAN (R 4.3.1)\n   inline         0.3.19   2021-05-31 [1] CRAN (R 4.3.0)\n   jsonlite       1.8.7    2023-06-29 [1] CRAN (R 4.3.1)\n   knitr          1.43     2023-05-25 [1] CRAN (R 4.3.0)\n   labeling       0.4.2    2020-10-20 [1] CRAN (R 4.3.0)\n   later          1.3.1    2023-05-02 [1] CRAN (R 4.3.0)\n   lattice        0.21-8   2023-04-05 [2] CRAN (R 4.3.0)\n   lazyeval     * 0.2.2    2019-03-15 [1] CRAN (R 4.3.0)\n   lifecycle      1.0.3    2022-10-07 [1] CRAN (R 4.3.0)\n   lme4           1.1-34   2023-07-04 [1] CRAN (R 4.3.1)\n   loo            2.6.0    2023-03-31 [1] CRAN (R 4.3.0)\n   magrittr       2.0.3    2022-03-30 [1] CRAN (R 4.3.0)\n   markdown       1.7      2023-05-16 [1] CRAN (R 4.3.0)\n   MASS           7.3-60   2023-05-04 [1] CRAN (R 4.3.0)\n   Matrix         1.6-0    2023-07-08 [1] CRAN (R 4.3.1)\n   matrixStats    1.0.0    2023-06-02 [1] CRAN (R 4.3.0)\n   mgcv           1.9-0    2023-07-11 [1] CRAN (R 4.3.0)\n   mime           0.12     2021-09-28 [1] CRAN (R 4.3.0)\n   miniUI         0.1.1.1  2018-05-18 [1] CRAN (R 4.3.0)\n   minqa          1.2.5    2022-10-19 [1] CRAN (R 4.3.0)\n   munsell        0.5.0    2018-06-12 [1] CRAN (R 4.3.0)\n   nlme           3.1-162  2023-01-31 [2] CRAN (R 4.3.0)\n   nloptr         2.0.3    2022-05-26 [1] CRAN (R 4.3.0)\n   pillar         1.9.0    2023-03-22 [1] CRAN (R 4.3.0)\n   pkgbuild       1.4.2    2023-06-26 [1] CRAN (R 4.3.1)\n   pkgconfig      2.0.3    2019-09-22 [1] CRAN (R 4.3.0)\n   plyr           1.8.8    2022-11-11 [1] CRAN (R 4.3.0)\n   prettyunits    1.1.1    2020-01-24 [1] CRAN (R 4.3.0)\n   processx       3.8.2    2023-06-30 [1] CRAN (R 4.3.1)\n   promises       1.2.0.1  2021-02-11 [1] CRAN (R 4.3.0)\n   ps             1.7.5    2023-04-18 [1] CRAN (R 4.3.0)\n   R6             2.5.1    2021-08-19 [1] CRAN (R 4.3.0)\n   ragg           1.2.5    2023-01-12 [1] CRAN (R 4.3.0)\n   Rcpp         * 1.0.11   2023-07-06 [1] CRAN (R 4.3.1)\n D RcppParallel   5.1.7    2023-02-27 [1] CRAN (R 4.3.0)\n   reshape2       1.4.4    2020-04-09 [1] CRAN (R 4.3.0)\n   rlang          1.1.1    2023-04-28 [1] CRAN (R 4.3.0)\n   rmarkdown      2.23     2023-07-01 [1] CRAN (R 4.3.0)\n   rstan          2.26.22  2023-05-02 [1] local\n   rstanarm     * 2.21.4   2023-04-08 [1] CRAN (R 4.3.0)\n   rstantools     2.3.1.1  2023-07-18 [1] CRAN (R 4.3.1)\n   rstudioapi     0.15.0   2023-07-07 [1] CRAN (R 4.3.1)\n   scales         1.2.1    2022-08-20 [1] CRAN (R 4.3.0)\n   sessioninfo    1.2.2    2021-12-06 [1] CRAN (R 4.3.0)\n   shiny          1.7.4.1  2023-07-06 [1] CRAN (R 4.3.1)\n   shinyjs        2.1.0    2021-12-23 [1] CRAN (R 4.3.0)\n   shinystan      2.6.0    2022-03-03 [1] CRAN (R 4.3.0)\n   shinythemes    1.2.0    2021-01-25 [1] CRAN (R 4.3.0)\n   StanHeaders    2.26.27  2023-06-14 [1] CRAN (R 4.3.1)\n   stringi        1.7.12   2023-01-11 [1] CRAN (R 4.3.0)\n   stringr        1.5.0    2022-12-02 [1] CRAN (R 4.3.0)\n   survival       3.5-5    2023-03-12 [2] CRAN (R 4.3.0)\n   systemfonts    1.0.4    2022-02-11 [1] CRAN (R 4.3.0)\n   textshaping    0.3.6    2021-10-13 [1] CRAN (R 4.3.0)\n   threejs        0.3.3    2020-01-21 [1] CRAN (R 4.3.0)\n   tibble         3.2.1    2023-03-20 [1] CRAN (R 4.3.0)\n   tidyselect     1.2.0    2022-10-10 [1] CRAN (R 4.3.0)\n   utf8           1.2.3    2023-01-31 [1] CRAN (R 4.3.0)\n   V8             4.3.3    2023-07-18 [1] CRAN (R 4.3.1)\n   vctrs          0.6.3    2023-06-14 [1] CRAN (R 4.3.1)\n   withr          2.5.0    2022-03-03 [1] CRAN (R 4.3.0)\n   xfun           0.39     2023-04-20 [1] CRAN (R 4.3.0)\n   xtable         1.8-4    2019-04-21 [1] CRAN (R 4.3.0)\n   xts            0.13.1   2023-04-16 [1] CRAN (R 4.3.0)\n   yaml           2.3.7    2023-01-23 [1] CRAN (R 4.3.0)\n   zoo            1.8-12   2023-04-13 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.0/library\n\n D ── DLL MD5 mismatch, broken installation.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-08-15-set-na-where-nonstandard-evaluation-use-case/index.html",
    "href": "posts/2017-08-15-set-na-where-nonstandard-evaluation-use-case/index.html",
    "title": "set_na_where(): a nonstandard evaluation use case",
    "section": "",
    "text": "In this post, I describe a recent case where I used rlang’s tidy evaluation system to do some data-cleaning. This example is not particularly involved, but it demonstrates is a basic but powerful idea: That we can capture the expressions that a user writes, pass them around as data, and make some 💫 magic ✨ happen. This technique in R is called nonstandard evaluation."
  },
  {
    "objectID": "posts/2017-08-15-set-na-where-nonstandard-evaluation-use-case/index.html#strange-eyetracking-data",
    "href": "posts/2017-08-15-set-na-where-nonstandard-evaluation-use-case/index.html#strange-eyetracking-data",
    "title": "set_na_where(): a nonstandard evaluation use case",
    "section": "Strange eyetracking data",
    "text": "Strange eyetracking data\nLast week, I had to deal with a file with some eyetracking data from a sequence-learning experiment. The eyetracker records the participant’s gaze location at a rate of 60 frames per second—except for this weird file which wrote out ~80 frames each second. In this kind of data, we have one row per eyetracking sample, and each sample records a timestamp and the gaze location :eyes: on the computer screen at each timestamp. In this particular dataset, we have x and y gaze coordinates in pixels (both eyes averaged together, GazeX and GazeY) or in screen proportions (for each eye in the EyeCoord columns.)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rlang)\n# the data is bundled with an R package I wrote\n# devtools::install_github(\"tjmahr/fillgaze\")\n\ndf &lt;- system.file(\"test-gaze.csv\", package = \"fillgaze\") %&gt;% \n  readr::read_csv() %&gt;% \n  mutate(Time = Time - min(Time)) %&gt;% \n  select(Time:REyeCoordY) %&gt;% \n  round(3) %&gt;% \n  mutate_at(vars(Time), round, 1) %&gt;% \n  mutate_at(vars(GazeX, GazeY), round, 0)\ndf\n#&gt; # A tibble: 14,823 × 8\n#&gt;     Time Trial GazeX GazeY LEyeCoordX LEyeCoordY REyeCoordX REyeCoordY\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1   0       1  1176   643      0.659      0.589      0.566      0.602\n#&gt;  2   3.5     1 -1920 -1080     -1         -1         -1         -1    \n#&gt;  3  20.2     1 -1920 -1080     -1         -1         -1         -1    \n#&gt;  4  36.8     1  1184   648      0.664      0.593      0.57       0.606\n#&gt;  5  40       1  1225   617      0.685      0.564      0.591      0.579\n#&gt;  6  56.7     1 -1920 -1080     -1         -1         -1         -1    \n#&gt;  7  73.4     1  1188   641      0.665      0.587      0.572      0.6  \n#&gt;  8  76.6     1  1204   621      0.674      0.568      0.58       0.582\n#&gt;  9  93.3     1 -1920 -1080     -1         -1         -1         -1    \n#&gt; 10 110.      1  1189   665      0.666      0.609      0.572      0.622\n#&gt; # ℹ 14,813 more rows\n\nIn this particular eyetracking setup, offscreen looks are coded as negative gaze coordinates, and what’s extra weird here is that every second or third point is incorrectly placed offscreen. We see that in the frequent -1920 values in GazeX. Plotting the first few x and y pixel locations shows the pattern as well.\n\np &lt;- ggplot(head(df, 40)) + \n  aes(x = Time) + \n  geom_hline(yintercept = 0, size = 2, color = \"white\") + \n  geom_point(aes(y = GazeX, color = \"GazeX\")) +\n  geom_point(aes(y = GazeY, color = \"GazeY\")) + \n  labs(\n    x = \"Time (ms)\", \n    y = \"Screen location (pixels)\", \n    color = \"Variable\"\n  )\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\np + \n  annotate(\n    \"text\", x = 50, y = -200, \n    label = \"offscreen\", color = \"grey20\"\n  ) + \n  annotate(\n    \"text\", x = 50, y = 200, \n    label = \"onscreen\", color = \"grey20\"\n  ) \n\n\n\nOffscreens looks occurred every two or three samples.\n\n\n\nIt is physiologically impossible for a person’s gaze to oscillate so quickly and with such magnitude (the gaze is tracked on a large screen display), so obviously something weird was going on with the experiment software.\nThis file motivated me to develop a general purpose package for interpolating missing data in eyetracking experiments. This package was always something I wanted to do, and this file moved it from the someday list to the today list."
  },
  {
    "objectID": "posts/2017-08-15-set-na-where-nonstandard-evaluation-use-case/index.html#a-function-to-recode-values-in-many-columns-as-na",
    "href": "posts/2017-08-15-set-na-where-nonstandard-evaluation-use-case/index.html#a-function-to-recode-values-in-many-columns-as-na",
    "title": "set_na_where(): a nonstandard evaluation use case",
    "section": "A function to recode values in many columns as NA\n",
    "text": "A function to recode values in many columns as NA\n\nThe first step in handling this problematic dataset is to convert the offscreen values into actual missing (NA) values). Because we have several columns of data, I wanted a succinct way to recode values in multiple columns into NA values.\nFirst, we sketch out the code we want to write when we’re done.\n\nset_na_where &lt;- function(data, ...) {\n  # do things\n}\n\nset_na_where(\n  data = df,\n  GazeX = GazeX &lt; -500 | 2200 &lt; GazeX,\n  GazeY = GazeY &lt; -200 | 1200 &lt; GazeY\n)\n\nThat is, after specifying the data, we list off an arbitrary number of column names, and with each name, we provide a rule to determine whether a value in that column is offscreen and should be set to NA. For example, we want every value in GazeX where GazeX &lt; -500 or 2299 &lt; GazeX is TRUE to be replaced with NA.\nBottling up magic spells\nLines of computer code are magic spells: We say the incantations and things happen around us. Put more formally, the code contains expressions that are evaluated in an environment.\n\nhey &lt;- \"Hello!\"\nmessage(hey)\n#&gt; Hello!\n\nexists(\"x\")\n#&gt; [1] FALSE\n\nx &lt;- pi ^ 2\nexists(\"x\")\n#&gt; [1] TRUE\n\nprint(x)\n#&gt; [1] 9.869604\n\nstop(\"what are you doing?\")\n#&gt; Error in eval(expr, envir, enclos): what are you doing?\n\nIn our function signature, function(data, ...), the expressions are collected in the special “dots” argument (...). In normal circumstances, we can view the contents of the dots by storing them in a list. Consider:\n\nhello_dots &lt;- function(...) {\n  str(list(...))\n}\nhello_dots(x = pi, y = 1:10, z = NA)\n#&gt; List of 3\n#&gt;  $ x: num 3.14\n#&gt;  $ y: int [1:10] 1 2 3 4 5 6 7 8 9 10\n#&gt;  $ z: logi NA\n\nBut we not passing in regular data, but expressions that need to be evaluated in a particular location. Below the magic words are uttered and we get an error because they mention things that do not exist in the current environment.\n\nhello_dots(GazeX = GazeX &lt; -500 | 2200 &lt; GazeX)\n#&gt; Error in eval(expr, envir, enclos): object 'GazeX' not found\n\nWhat we need to do is prevent these words from being uttered until the time and place are right. Nonstandard evaluation is a way of bottling up magic spells and changing how or where they are cast—sometimes we even change the magic words themselves. We bottle up or capture the expressions given by the user by quoting them. quo() quotes a single expression, and quos() (plural) will quote a list of expressions. Below, we capture the expressions stored in the dots :speech_balloon: and then make sure that their names match column names in the dataframe.\n\nset_na_where &lt;- function(data, ...) {\n  dots &lt;- quos(...)\n  stopifnot(names(dots) %in% names(data), !anyDuplicated(names(dots)))\n  \n  dots\n  # more to come\n}\n\nspells &lt;- set_na_where(\n  data = df,\n  GazeX = GazeX &lt; -500 | 2200 &lt; GazeX, \n  GazeY = GazeY &lt; -200 | 1200 &lt; GazeY\n)\nspells\n#&gt; &lt;list_of&lt;quosure&gt;&gt;\n#&gt; \n#&gt; $GazeX\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^GazeX &lt; -500 | 2200 &lt; GazeX\n#&gt; env:  global\n#&gt; \n#&gt; $GazeY\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^GazeY &lt; -200 | 1200 &lt; GazeY\n#&gt; env:  global\n\nI call these results spells because it just contains the expressions stored as data. We can interrogate these results like data. We can query the names of the stored data, and we can extract values (the quoted expressions).\n\nnames(spells)\n#&gt; [1] \"GazeX\" \"GazeY\"\nspells[[1]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^GazeX &lt; -500 | 2200 &lt; GazeX\n#&gt; env:  global\n\nCasting spells\nWe can cast a spell by evaluating an expression. To keep the incantation from fizzling out, we specify that we want to evaluate the expression inside of the dataframe. The function eval_tidy(expr, data) lets us do just that: evaluate an expression expr inside of some data.\n\n# Evaluate the first expression inside of the data\nxs_to_set_na &lt;- eval_tidy(spells[[1]], data = df)\n\n# Just the first few bc there are 10000+ values\nxs_to_set_na[1:20]\n#&gt;  [1] FALSE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE\n#&gt; [13] FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE\n\nIn fact, we can evaluate them all at once with by applying eval_tidy() on each listed expression.\n\nto_set_na &lt;- lapply(spells, eval_tidy, data = df)\nstr(to_set_na)\n#&gt; List of 2\n#&gt;  $ GazeX: logi [1:14823] FALSE TRUE TRUE FALSE FALSE TRUE ...\n#&gt;  $ GazeY: logi [1:14823] FALSE TRUE TRUE FALSE FALSE TRUE ...\n\nFinishing touches\nNow, the rest of the function is straightforward. Evaluate each NA-rule on the named columns, and then set each row where the rule is TRUE to NA.\n\nset_na_where &lt;- function(data, ...) {\n  dots &lt;- quos(...)\n  stopifnot(names(dots) %in% names(data), !anyDuplicated(names(dots)))\n  \n  set_to_na &lt;- lapply(dots, eval_tidy, data = data)\n  \n  for (col in names(set_to_na)) {\n    data[set_to_na[[col]], col] &lt;- NA\n  }\n  \n  data\n}\n\nresults &lt;- set_na_where(\n  data = df,\n  GazeX = GazeX &lt; -500 | 2200 &lt; GazeX, \n  GazeY = GazeY &lt; -200 | 1200 &lt; GazeY\n)\nresults\n#&gt; # A tibble: 14,823 × 8\n#&gt;     Time Trial GazeX GazeY LEyeCoordX LEyeCoordY REyeCoordX REyeCoordY\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1   0       1  1176   643      0.659      0.589      0.566      0.602\n#&gt;  2   3.5     1    NA    NA     -1         -1         -1         -1    \n#&gt;  3  20.2     1    NA    NA     -1         -1         -1         -1    \n#&gt;  4  36.8     1  1184   648      0.664      0.593      0.57       0.606\n#&gt;  5  40       1  1225   617      0.685      0.564      0.591      0.579\n#&gt;  6  56.7     1    NA    NA     -1         -1         -1         -1    \n#&gt;  7  73.4     1  1188   641      0.665      0.587      0.572      0.6  \n#&gt;  8  76.6     1  1204   621      0.674      0.568      0.58       0.582\n#&gt;  9  93.3     1    NA    NA     -1         -1         -1         -1    \n#&gt; 10 110.      1  1189   665      0.666      0.609      0.572      0.622\n#&gt; # ℹ 14,813 more rows\n\nVisually, we can see that the offscreen values are no longer plotted. Plus, we are told that our data now has missing values.\n\n# `plot %+% data`: replace the data in `plot` with `data`\np %+% head(results, 40)\n#&gt; Warning: Removed 15 rows containing missing values (`geom_point()`).\n#&gt; Removed 15 rows containing missing values (`geom_point()`).\n\n\n\nOffscreens are no longer plotted.\n\n\n\nOne of the quirks about some eyetracking data is that during a blink, sometimes the device will record the x location but not the y location. (I think this happens because blinks move vertically so the horizontal detail can still be inferred in a half-closed eye.) This effect shows up in the data when there are more NA values for the y values than for the x values:\n\ncount_na &lt;- function(data, ...) {\n  subset &lt;- select(data, ...)\n  lapply(subset, function(xs) sum(is.na(xs)))\n}\n\ncount_na(results, GazeX, GazeY)\n#&gt; $GazeX\n#&gt; [1] 2808\n#&gt; \n#&gt; $GazeY\n#&gt; [1] 3064\n\nWe can equalize these counts by running the function a second time with new rules.\n\ndf %&gt;% \n  set_na_where(\n    GazeX = GazeX &lt; -500 | 2200 &lt; GazeX, \n    GazeY = GazeY &lt; -200 | 1200 &lt; GazeY\n  ) %&gt;% \n  set_na_where(\n    GazeX = is.na(GazeY), \n    GazeY = is.na(GazeX)\n  ) %&gt;% \n  count_na(GazeX, GazeY)\n#&gt; $GazeX\n#&gt; [1] 3069\n#&gt; \n#&gt; $GazeY\n#&gt; [1] 3069\n\nAlternatively, we can do this all at once by using the same NA-filtering rule on GazeX and GazeY.\n\ndf %&gt;% \n  set_na_where(\n    GazeX = GazeX &lt; -500 | 2200 &lt; GazeX | GazeY &lt; -200 | 1200 &lt; GazeY, \n    GazeY = GazeX &lt; -500 | 2200 &lt; GazeX | GazeY &lt; -200 | 1200 &lt; GazeY\n  ) %&gt;% \n  count_na(GazeX, GazeY)\n#&gt; $GazeX\n#&gt; [1] 3069\n#&gt; \n#&gt; $GazeY\n#&gt; [1] 3069\n\nThese last examples, where we compare different rules, showcases how nonstandard evaluation lets us write in a very succinct and convenient manner and quickly iterate over possible rules. Works like magic, indeed.\n\n\n\n\n\n\n\nSession info\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21 ucrt)\n os       Windows 11 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-28\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date (UTC) lib source\n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.3.1)\n bit           4.0.5      2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5      2020-08-30 [1] CRAN (R 4.3.0)\n cli           3.6.1      2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0      2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n digest        0.6.33     2023-07-07 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2      2023-04-20 [1] CRAN (R 4.3.0)\n emo           0.0.0.9000 2023-07-27 [1] Github (hadley/emo@3f03b11)\n evaluate      0.21       2023-05-05 [1] CRAN (R 4.3.0)\n fansi         1.0.4      2023-01-22 [1] CRAN (R 4.3.0)\n farver        2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n generics      0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2      2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2      2022-02-24 [1] CRAN (R 4.3.0)\n gtable        0.3.3      2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5      2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2      2023-03-17 [1] CRAN (R 4.3.0)\n jsonlite      1.8.7      2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43       2023-05-25 [1] CRAN (R 4.3.0)\n labeling      0.4.2      2020-10-20 [1] CRAN (R 4.3.0)\n lifecycle     1.0.3      2022-10-07 [1] CRAN (R 4.3.0)\n lubridate     1.9.2      2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n munsell       0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n purrr         1.0.1      2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n ragg          1.2.5      2023-01-12 [1] CRAN (R 4.3.0)\n readr         2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n rlang       * 1.1.1      2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.23       2023-07-01 [1] CRAN (R 4.3.0)\n rstudioapi    0.15.0     2023-07-07 [1] CRAN (R 4.3.1)\n scales        1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringi       1.7.12     2023-01-11 [1] CRAN (R 4.3.0)\n stringr       1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n systemfonts   1.0.4      2022-02-11 [1] CRAN (R 4.3.0)\n textshaping   0.3.6      2021-10-13 [1] CRAN (R 4.3.0)\n tibble        3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n timechange    0.2.0      2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0      2023-05-12 [1] CRAN (R 4.3.0)\n utf8          1.2.3      2023-01-31 [1] CRAN (R 4.3.0)\n vctrs         0.6.3      2023-06-14 [1] CRAN (R 4.3.1)\n vroom         1.6.3      2023-04-28 [1] CRAN (R 4.3.0)\n withr         2.5.0      2022-03-03 [1] CRAN (R 4.3.0)\n xfun          0.39       2023-04-20 [1] CRAN (R 4.3.0)\n yaml          2.3.7      2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.0/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2021-02-05-lists-knitr-secret-weapon/index.html",
    "href": "posts/2021-02-05-lists-knitr-secret-weapon/index.html",
    "title": "Lists are my secret weapon for reporting stats with knitr",
    "section": "",
    "text": "I am going to describe my favorite knitr trick: Using lists to simplify inline reporting. Trick might not do it justice. I consider this a best practice for working with knitr."
  },
  {
    "objectID": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#plug-it-in",
    "href": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#plug-it-in",
    "title": "Lists are my secret weapon for reporting stats with knitr",
    "section": "Plug it in",
    "text": "Plug it in\nInline reporting lets you insert R expressions inside of markdown text. Those expressions are evaluated and their results are plugged in as text. The following example shows a common use case: Reporting descriptive statistics. The gamair::sitka dataset describes the longitudinal growth of Sitka spruce trees in different ozone conditions, so here are some lines we might report:\n\n\n\n\n```{r}\nlibrary(magrittr)\ndata(\"sitka\", package = \"gamair\")\nn_trees &lt;- length(levels(sitka$id.num))\nn_conditions &lt;- length(unique(sitka$ozone))\n```\n\nThe dataset contains `r nrow(sitka)` tree size measurements \nfrom `r n_trees` trees grown in \n`r n_conditions` ozone treatment conditions.\nwhich produces\nThe dataset contains 1027 tree size measurements \nfrom 0 trees grown in \n2 ozone treatment conditions.\nIf we update the dataset, the numbers will update automatically when the document is reknitted. It’s just magical. Besides reporting statistics, I routinely use inline reporting for package versions, dates, file provenance, links to package documentation, and emoji. Here is an example of each:\n```{r}\nknitted_when &lt;- format(Sys.Date())\nknitted_where &lt;- knitr::current_input()\nknitted_with &lt;- packageVersion(\"knitr\")\nknitted_doc_url &lt;- downlit::autolink_url(\"knitr::knit()\")\n```\n\nReported prepared on `r knitted_when` from ``r knitted_where`` \nwith knitr version `r knitted_with` `r emo::ji('smile')`. \nRead more about [`knitr::knit()`](`r knitted_doc_url`)`. \n\n::::\n\nReported prepared on 2023-07-27 from `index.rmarkdown` \nwith knitr version 1.43 😄.\nRead more about [`knitr::knit()`](https://rdrr.io/pkg/knitr/man/knit.html)`."
  },
  {
    "objectID": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#are-your-variable-names-doing-a-lists-job",
    "href": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#are-your-variable-names-doing-a-lists-job",
    "title": "Lists are my secret weapon for reporting stats with knitr",
    "section": "Are your variable names doing a list’s job?",
    "text": "Are your variable names doing a list’s job?\nIn this last example, I used prefixes in variable names to convey that the data were related. knitted_when, knitted_where and knitted_with are all facts about the knitting process. They are all reported in the narrative text pretty close to each other. The prefix informally bundles them together. The prefix also helps with writing our code because we have to remember less. We can type knitted_ and press Tab and let autocompletion remind us which variables are available.\n\n\n# Simulate tab completion (what happens when we press Tab in after \n# `knitted_` in RStudio). The definition of `tab()` is not important.\n\"knitted_\" %&gt;% tab()\n#&gt; knitted_doc_url\n#&gt; knitted_when\n#&gt; knitted_where\n#&gt; knitted_with\n\nBut—and here is the key insight—what if we change that underscore _ into a dollar sign $, so to speak? That is, let’s bundle everything into a list and then report the results by accessing list elements.\n```{r}\nknitted &lt;- list(\n  when = format(Sys.Date()),\n  where = knitr::current_input(),\n  with = packageVersion(\"knitr\"),\n  doc_url = downlit::autolink_url(\"knitr::knit()\")\n)\n```\n\nReported prepared on `r knitted$when` from ``r knitted$where`` \nwith knitr version `r knitted$with` `r emo::ji('happy')`. \nRead more about [`knitr::knit()`](`r knitted$doc_url`)`. \n\n::::\n\nReported prepared on 2023-07-27 from `index.rmarkdown` \nwith knitr version 1.43 😄. \nRead more about [`knitr::knit()`](https://rdrr.io/pkg/knitr/man/knit.html)`. \nWe have structured names, and we still retain our Tab completion:\n\n\"knitted$\" %&gt;% tab()\n#&gt; knitted$when\n#&gt; knitted$where\n#&gt; knitted$with\n#&gt; knitted$doc_url\n\nBut we can also glance at our list to see everything about the knitting process all at once.\n\nknitted\n#&gt; $when\n#&gt; [1] \"2023-07-27\"\n#&gt; \n#&gt; $where\n#&gt; [1] \"index.rmarkdown\"\n#&gt; \n#&gt; $with\n#&gt; [1] '1.43'\n#&gt; \n#&gt; $doc_url\n#&gt; [1] \"https://rdrr.io/pkg/knitr/man/knit.html\"\n\nBasically, using a list formalizes the relationship we had implicitly set out by using our naming convention, but so what? How does this help inline reporting? Lists have all the nice benefits of using a naming convention, plus one important feature: We can create lists programmatically."
  },
  {
    "objectID": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#set-up-model-results-with-tidy",
    "href": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#set-up-model-results-with-tidy",
    "title": "Lists are my secret weapon for reporting stats with knitr",
    "section": "Set up model results with tidy()\n",
    "text": "Set up model results with tidy()\n\nLet’s say we model the growth of each tree in each ozone condition and want to know how much steeper the growth rate is for the ozone treatment condition. We fit a linear mixed model where we estimate the population averages for the intercept and slope for each ozone condition, and we use random effects to accord each tree its own intercept and slope.\n\nlibrary(lme4)\n#&gt; Warning: package 'lme4' was built under R version 4.3.1\n#&gt; Loading required package: Matrix\n#&gt; Warning: package 'Matrix' was built under R version 4.3.1\n#&gt; \n#&gt; Attaching package: 'Matrix'\n#&gt; The following objects are masked from 'package:tidyr':\n#&gt; \n#&gt;     expand, pack, unpack\n# Rescale to get improve convergence\nsitka$hund_days &lt;- sitka$days / 100\nm &lt;- lmer(\n  log.size ~ hund_days * ozone + (hund_days | id.num),\n  sitka\n) \nsummary(m)\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: log.size ~ hund_days * ozone + (hund_days | id.num)\n#&gt;    Data: sitka\n#&gt; \n#&gt; REML criterion at convergence: 767\n#&gt; \n#&gt; Scaled residuals: \n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.6836 -0.4723  0.0078  0.4237  2.5919 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev. Corr \n#&gt;  id.num   (Intercept) 0.390928 0.62524       \n#&gt;           hund_days   0.002459 0.04959  -0.23\n#&gt;  Residual             0.082770 0.28770       \n#&gt; Number of obs: 1027, groups:  id.num, 79\n#&gt; \n#&gt; Fixed effects:\n#&gt;                 Estimate Std. Error t value\n#&gt; (Intercept)      4.25491    0.13100  32.481\n#&gt; hund_days        0.33903    0.01278  26.528\n#&gt; ozone           -0.14101    0.15844  -0.890\n#&gt; hund_days:ozone -0.03611    0.01546  -2.336\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;             (Intr) hnd_dy ozone \n#&gt; hund_days   -0.345              \n#&gt; ozone       -0.827  0.286       \n#&gt; hund_dys:zn  0.286 -0.827 -0.345\n\nOur job is get to numbers from this summary view into prose. For this example, we want to report that the two groups don’t have a statistically clear difference in their intercepts, as given by the ozone line in the model summary. We also want to report that growth per 100 days is statistically significantly slower in the ozone group hund_days:ozone.\nFirst, we tidy() the model summary using broom.mixed.\n\nlibrary(tidyverse)\nlibrary(broom.mixed)\ntidy(m, conf.int = TRUE) %&gt;% \n  filter(effect == \"fixed\") \n#&gt; # A tibble: 4 × 8\n#&gt;   effect group term            estimate std.error statistic conf.low conf.high\n#&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 fixed  &lt;NA&gt;  (Intercept)       4.25      0.131     32.5     4.00     4.51   \n#&gt; 2 fixed  &lt;NA&gt;  hund_days         0.339     0.0128    26.5     0.314    0.364  \n#&gt; 3 fixed  &lt;NA&gt;  ozone            -0.141     0.158     -0.890  -0.452    0.170  \n#&gt; 4 fixed  &lt;NA&gt;  hund_days:ozone  -0.0361    0.0155    -2.34   -0.0664  -0.00581\n\nWe are also going to format the numbers. I have my own R package for this job called printy. Below I use it to round numbers—round() drops 0s off the ends of rounded numbers whereas printy::fmt_fixed_digits() keeps them. I also use it for formatting minus signs.\n\ntext_ready &lt;- tidy(m, conf.int = TRUE) %&gt;% \n  filter(effect == \"fixed\") %&gt;% \n  mutate(\n    # round the numbers\n    across(\n      c(estimate, conf.low, conf.high), \n      printy::fmt_fix_digits, \n      2\n    ),\n    se = printy::fmt_fix_digits(std.error, 3),\n    # use a minus sign instead of a hyphen for negative numbers\n    across(\n      c(estimate, conf.low, conf.high), \n      printy::fmt_minus_sign\n    ),\n    ci = glue::glue(\"[{conf.low}, {conf.high}]\")\n  ) %&gt;% \n  select(term, estimate, se, ci)\n#&gt; Warning: There was 1 warning in `mutate()`.\n#&gt; ℹ In argument: `across(...)`.\n#&gt; Caused by warning:\n#&gt; ! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\n#&gt; Supply arguments directly to `.fns` through an anonymous function instead.\n#&gt; \n#&gt;   # Previously\n#&gt;   across(a:b, mean, na.rm = TRUE)\n#&gt; \n#&gt;   # Now\n#&gt;   across(a:b, \\(x) mean(x, na.rm = TRUE))\ntext_ready\n#&gt; # A tibble: 4 × 4\n#&gt;   term            estimate    se    ci                        \n#&gt;   &lt;chr&gt;           &lt;chr&gt;       &lt;chr&gt; &lt;glue&gt;                    \n#&gt; 1 (Intercept)     4.25        0.131 [4.00, 4.51]              \n#&gt; 2 hund_days       0.34        0.013 [0.31, 0.36]              \n#&gt; 3 ozone           &minus;0.14 0.158 [&minus;0.45, 0.17]       \n#&gt; 4 hund_days:ozone &minus;0.04 0.015 [&minus;0.07, &minus;0.01]\n\n\n\n\nWe could use dataframe functions to filter() down to the down the terms and pull() the values and use a list:\n```{r}\nstats &lt;- list()\nstats$b_intercept &lt;- text_ready %&gt;% \n  filter(term == \"(Intercept)\") %&gt;% \n  pull(estimate)\n```\n\nThe average log-size in the control condition was `r stats$b_intercept` units.\n\n::::\n\nThe average log-size in the control condition was 4.25 units.\n(The documentation for sitka$log.size doesn’t say what units the data are in, so I’m sticking with “units” 🙃.)"
  },
  {
    "objectID": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#split-makes-lists",
    "href": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#split-makes-lists",
    "title": "Lists are my secret weapon for reporting stats with knitr",
    "section": "\nsplit() makes lists",
    "text": "split() makes lists\nA much better approach is to use split() to create a list using the values in a dataframe column. To make the list easier for typing, I use janitor::make_clean_names() to clean up the term value.\n\nstats &lt;- text_ready %&gt;% \n  mutate(term = janitor::make_clean_names(term)) %&gt;%\n  split(.$term)\n\nNow we have a list of one-row dataframes:\n\nstr(stats)\n#&gt; List of 4\n#&gt;  $ hund_days      : tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ term    : chr \"hund_days\"\n#&gt;   ..$ estimate: chr \"0.34\"\n#&gt;   ..$ se      : chr \"0.013\"\n#&gt;   ..$ ci      : 'glue' chr \"[0.31, 0.36]\"\n#&gt;  $ hund_days_ozone: tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ term    : chr \"hund_days_ozone\"\n#&gt;   ..$ estimate: chr \"&minus;0.04\"\n#&gt;   ..$ se      : chr \"0.015\"\n#&gt;   ..$ ci      : 'glue' chr \"[&minus;0.07, &minus;0.01]\"\n#&gt;  $ intercept      : tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ term    : chr \"intercept\"\n#&gt;   ..$ estimate: chr \"4.25\"\n#&gt;   ..$ se      : chr \"0.131\"\n#&gt;   ..$ ci      : 'glue' chr \"[4.00, 4.51]\"\n#&gt;  $ ozone          : tibble [1 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ term    : chr \"ozone\"\n#&gt;   ..$ estimate: chr \"&minus;0.14\"\n#&gt;   ..$ se      : chr \"0.158\"\n#&gt;   ..$ ci      : 'glue' chr \"[&minus;0.45, 0.17]\"\n\nAnd we have structured, autocomplete-friendly names too:\n\n\"stats$\" %&gt;% tab()\n#&gt; stats$hund_days\n#&gt; stats$hund_days_ozone\n#&gt; stats$intercept\n#&gt; stats$ozone\n\n\"stats$ozone$\" %&gt;% tab()\n#&gt; stats$ozone$term\n#&gt; stats$ozone$estimate\n#&gt; stats$ozone$se\n#&gt; stats$ozone$ci\n\nNow, we can write up our results with inline reporting:\n```{r}\nstats &lt;- text_ready %&gt;% \n  mutate(term = janitor::make_clean_names(term)) %&gt;%\n  split(.$term)\n```\n\nThe average log-size in the control condition was \n`r stats$intercept$estimate` units, 95% Wald CI `r stats$intercept$ci`. \nThere was not a statistically clear difference between the \nozone conditions for their intercepts (day-0 values), \n*B* = `r stats$ozone$estimate`, `r stats$ozone$ci`.\nFor the control group, the average growth rate was \n`r stats$hund_days$estimate` log-size units per 100 days, \n`r stats$hund_days$ci`. The growth rate for \nthe ozone treatment group was significantly slower, \n*diff* = `r stats$hund_days_ozone$estimate`, \n`r stats$hund_days_ozone$ci`.\n\n::::\n\nThe average log-size in the control condition was \n4.25 units, 95% Wald CI [4.00, 4.51]. \nThere was not a statistically clear difference between the \nozone conditions for their intercepts (day-0 values), \n*B* = &minus;0.14, [&minus;0.45, 0.17].\nFor the control group, the average growth rate was \n0.34 log-size units per 100 days, \n[0.31, 0.36]. The growth rate for the ozone treatment group was \nsignificantly slower, *diff* = &minus;0.04, \n[&minus;0.07, &minus;0.01].\nIsn’t that RMarkdown text just a joy to read? Everything so neatly named and organized, and we got all of that for free by using tidy() and split() to make a list.\n\n\n\n\n\n\nWrap it up, put a bow on it\n\n\n\nBy the way, we can also make the RMarkdown source even neater by using my WrapRmd RStudio plugin which wraps the text so lines of text are all less than a given width. Some other tools for rewrapping markdown text will insert line breaks inside of spans of inline reporting and break them. I have my RStudio set so that Ctrl+Shift+Alt+/ will rewrap the RMarkdown text."
  },
  {
    "objectID": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#splitting-splits-of-splits",
    "href": "posts/2021-02-05-lists-knitr-secret-weapon/index.html#splitting-splits-of-splits",
    "title": "Lists are my secret weapon for reporting stats with knitr",
    "section": "Splitting splits of splits",
    "text": "Splitting splits of splits\nI am such a champion of this approach that I wrote my own split function for splitting by multiple variables. In the datasets::mtcars dataset, suppose we want to report the mean mpg of 6- and 8-cylinder (cyl) vehicles split by automatic versus manual (am) vehicles. We compute the stats with some basic dplyring and we prepare names that work better with split().\n\ncar_means &lt;- mtcars %&gt;%\n  group_by(cyl, am) %&gt;%\n  summarise(\n    n = n(), \n    mean_mpg = mean(mpg), \n    .groups = \"drop\"\n  ) %&gt;% \n  # make names for spliting\n  mutate(\n    a = paste0(\"am_\", am),\n    c = paste0(\"cyl_\", cyl),\n  )\ncar_means\n#&gt; # A tibble: 6 × 6\n#&gt;     cyl    am     n mean_mpg a     c    \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1     4     0     3     22.9 am_0  cyl_4\n#&gt; 2     4     1     8     28.1 am_1  cyl_4\n#&gt; 3     6     0     4     19.1 am_0  cyl_6\n#&gt; 4     6     1     3     20.6 am_1  cyl_6\n#&gt; 5     8     0    12     15.0 am_0  cyl_8\n#&gt; 6     8     1     2     15.4 am_1  cyl_8\n\nNow enter super_split():\n\ncar_stats &lt;- car_means %&gt;% \n  printy::super_split(a, c)\n\n# set `max.level` to not print individual tibble structures\nstr(car_stats, max.level = 3)\n#&gt; List of 2\n#&gt;  $ am_0:List of 3\n#&gt;   ..$ cyl_4: tibble [1 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ cyl_6: tibble [1 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ cyl_8: tibble [1 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ am_1:List of 3\n#&gt;   ..$ cyl_4: tibble [1 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ cyl_6: tibble [1 × 6] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ cyl_8: tibble [1 × 6] (S3: tbl_df/tbl/data.frame)\n\nHere, we have a list of lists of 1-row dataframes, and we can just use $ to drill down the lists during inline reporting.\nThe average mpg of the `r car_stats$am_0$cyl_4$n` automatic, \nfour-cylinder cars was `r car_stats$am_0$cyl_4$mean_mpg`.\n\n::::\n\nThe average mpg of the 3 automatic, \nfour-cylinder cars was 22.9.\nFor the curious, super_split() works behind the scenes by exploiting two functions:\n\n\nsplit() adds a level of depth to a list by splitting a list into sublists using a variable.\n\npurrr::map_depth() applies a function .f on the lists at a given .depth.\n\nSo the function walks through each variable and applies split() at successively deeper depths.\n\nsuper_split &lt;- function(.data, ...) {\n  dots &lt;- rlang::enquos(...)\n  for (var in seq_along(dots)) {\n    var_name &lt;- rlang::as_name(dots[[var]])\n    .data &lt;- purrr::map_depth(\n      .x = .data,\n      .depth = var - 1,\n      .f = function(xs) split(xs, xs[var_name])\n    )\n  }\n  .data\n}\n\nThe first variable splits the list at depth 0, the second variable splits the sublists at depth 1 (which were created in the prior split), and so on. The business with enquos() is there to let me refer to the variable names directly.\n\nSession info:\n\n.session_info\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────\n#&gt;  setting  value\n#&gt;  version  R version 4.3.0 (2023-04-21 ucrt)\n#&gt;  os       Windows 11 x64 (build 22621)\n#&gt;  system   x86_64, mingw32\n#&gt;  ui       RTerm\n#&gt;  language (EN)\n#&gt;  collate  English_United States.utf8\n#&gt;  ctype    English_United States.utf8\n#&gt;  tz       America/Chicago\n#&gt;  date     2023-07-27\n#&gt;  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  package     * version    date (UTC) lib source\n#&gt;  assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.3.1)\n#&gt;  backports     1.4.1      2021-12-13 [1] CRAN (R 4.3.0)\n#&gt;  boot          1.3-28.1   2022-11-22 [2] CRAN (R 4.3.0)\n#&gt;  broom         1.0.5      2023-06-09 [1] CRAN (R 4.3.1)\n#&gt;  broom.mixed * 0.2.9.4    2022-04-17 [1] CRAN (R 4.3.0)\n#&gt;  cachem        1.0.8      2023-05-01 [1] CRAN (R 4.3.0)\n#&gt;  cli           3.6.1      2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;  codetools     0.2-19     2023-02-01 [2] CRAN (R 4.3.0)\n#&gt;  colorspace    2.1-0      2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;  crayon        1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n#&gt;  digest        0.6.33     2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;  downlit       0.4.3      2023-06-29 [1] CRAN (R 4.3.0)\n#&gt;  dplyr       * 1.1.2      2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;  emo           0.0.0.9000 2023-07-27 [1] Github (hadley/emo@3f03b11)\n#&gt;  evaluate      0.21       2023-05-05 [1] CRAN (R 4.3.0)\n#&gt;  fansi         1.0.4      2023-01-22 [1] CRAN (R 4.3.0)\n#&gt;  fastmap       1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  forcats     * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n#&gt;  furrr         0.3.1      2022-08-15 [1] CRAN (R 4.3.0)\n#&gt;  future        1.33.0     2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;  generics      0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggplot2     * 3.4.2      2023-04-03 [1] CRAN (R 4.3.0)\n#&gt;  globals       0.16.2     2022-11-21 [1] CRAN (R 4.3.0)\n#&gt;  glue          1.6.2      2022-02-24 [1] CRAN (R 4.3.0)\n#&gt;  gtable        0.3.3      2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;  hms           1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;  htmltools     0.5.5      2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;  htmlwidgets   1.6.2      2023-03-17 [1] CRAN (R 4.3.0)\n#&gt;  janitor       2.2.0      2023-02-02 [1] CRAN (R 4.3.0)\n#&gt;  jsonlite      1.8.7      2023-06-29 [1] CRAN (R 4.3.1)\n#&gt;  knitr         1.43       2023-05-25 [1] CRAN (R 4.3.0)\n#&gt;  lattice       0.21-8     2023-04-05 [2] CRAN (R 4.3.0)\n#&gt;  lifecycle     1.0.3      2022-10-07 [1] CRAN (R 4.3.0)\n#&gt;  listenv       0.9.0      2022-12-16 [1] CRAN (R 4.3.0)\n#&gt;  lme4        * 1.1-34     2023-07-04 [1] CRAN (R 4.3.1)\n#&gt;  lubridate   * 1.9.2      2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;  magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  MASS          7.3-60     2023-05-04 [1] CRAN (R 4.3.0)\n#&gt;  Matrix      * 1.6-0      2023-07-08 [1] CRAN (R 4.3.1)\n#&gt;  memoise       2.0.1      2021-11-26 [1] CRAN (R 4.3.0)\n#&gt;  minqa         1.2.5      2022-10-19 [1] CRAN (R 4.3.0)\n#&gt;  munsell       0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#&gt;  nlme          3.1-162    2023-01-31 [2] CRAN (R 4.3.0)\n#&gt;  nloptr        2.0.3      2022-05-26 [1] CRAN (R 4.3.0)\n#&gt;  parallelly    1.36.0     2023-05-26 [1] CRAN (R 4.3.0)\n#&gt;  pillar        1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  printy        0.0.0.9003 2023-07-12 [1] Github (tjmahr/printy@df0d96e)\n#&gt;  purrr       * 1.0.1      2023-01-10 [1] CRAN (R 4.3.0)\n#&gt;  R6            2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  ragg          1.2.5      2023-01-12 [1] CRAN (R 4.3.0)\n#&gt;  Rcpp          1.0.11     2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;  readr       * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;  rlang         1.1.1      2023-04-28 [1] CRAN (R 4.3.0)\n#&gt;  rmarkdown     2.23       2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;  rstudioapi    0.15.0     2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;  scales        1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n#&gt;  sessioninfo   1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#&gt;  snakecase     0.11.0     2019-05-25 [1] CRAN (R 4.3.0)\n#&gt;  stringi       1.7.12     2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;  stringr     * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n#&gt;  systemfonts   1.0.4      2022-02-11 [1] CRAN (R 4.3.0)\n#&gt;  textshaping   0.3.6      2021-10-13 [1] CRAN (R 4.3.0)\n#&gt;  tibble      * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidyr       * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n#&gt;  tidyselect    1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  tidyverse   * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n#&gt;  timechange    0.2.0      2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;  tzdb          0.4.0      2023-05-12 [1] CRAN (R 4.3.0)\n#&gt;  utf8          1.2.3      2023-01-31 [1] CRAN (R 4.3.0)\n#&gt;  vctrs         0.6.3      2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;  withr         2.5.0      2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;  xfun          0.39       2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;  yaml          2.3.7      2023-01-23 [1] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n#&gt;  [2] C:/Program Files/R/R-4.3.0/library\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-03-10-self-titled-ggplot2-plots/index.html",
    "href": "posts/2022-03-10-self-titled-ggplot2-plots/index.html",
    "title": "Self-documenting plots in ggplot2",
    "section": "",
    "text": "When I am showing off a plotting technique in ggplot2, I sometimes like to include the R code that produced the plot as part of the plot. Here is an example I made to demonstrate the debug parameter in element_text():\nlibrary(ggplot2)\n\nself_document(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = \"white\") +\n    labs(title = \"A basic histogram\") +\n    theme(axis.title = element_text(debug = TRUE))\n)\nLet’s call these “self-documenting plots”. If we’re feeling nerdy, we might also call them “qquines”, although they are not true quines.\nIn this post, we will build up a self_document() function from scratch. Here are the problems we need to sort out:"
  },
  {
    "objectID": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#creating-the-code-annotation",
    "href": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#creating-the-code-annotation",
    "title": "Self-documenting plots in ggplot2",
    "section": "Creating the code annotation",
    "text": "Creating the code annotation\nAs a first step, let’s just treat our plotting code as a string that is ready to use for annotation.\n\np_text &lt;- 'ggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(bins = 20, color = \"white\") +\n  labs(title = \"A basic histogram\")'\n\np_plot &lt;- ggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(bins = 20, color = \"white\") +\n  labs(title = \"A basic histogram\")\n\nIn order to have a titled plot along with this annotation, we need some way to combine these two graphical objects together (the code and the plot produced by ggplot2). I like the patchwork package for this job. Here we use wrap_elements() to capture the plot into a “patch” that patchwork can annotate.\n\nlibrary(patchwork)\nwrap_elements(p_plot) + \n  plot_annotation(title = p_text)\n\n\n\n\nLet’s style this title to use a monospaced font. I use Windows and like Consolas, so I will use that font.\n\n# Use default mono font if \"Consolas\" is not available\nextrafont::loadfonts(device = \"win\", quiet = TRUE)\nmonofont &lt;- ifelse(\n  extrafont::choose_font(\"Consolas\") == \"\", \n  \"mono\", \n  \"Consolas\"\n)\n\ntitle_theme &lt;- theme(\n  plot.title = element_text(\n    family = monofont, hjust = 0, size = rel(.9), \n    margin = margin(0, 0, 5.5, 0, unit = \"pt\")\n  )\n)\n\nwrap_elements(p_plot) + \n  plot_annotation(title = p_text, theme = title_theme)  \n\n\n\n\nOne problem with this setup is that the plotting code has to be edited in two places: the plot p_plot and the title p_text. As a result, it’s easy for these two pieces of code to fall out of sync with each other, turning our self-documenting plot into a lying liar plot.\nThe solution is pretty easy: Tell R that p_text is code with parse() and evaluate the code with eval():\n\nwrap_elements(eval(parse(text = p_text))) + \n  plot_annotation(title = p_text, theme = title_theme)  \n\n\n\n\nThis works. It gets the job done. But we find ourselves in a clumsy workflow, either having to edit R code inside of quotes or editing the plot interactively and then having to wrap it in quotes. Let’s do better."
  },
  {
    "objectID": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#capturing-plotting-code-as-a-string",
    "href": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#capturing-plotting-code-as-a-string",
    "title": "Self-documenting plots in ggplot2",
    "section": "Capturing plotting code as a string",
    "text": "Capturing plotting code as a string\nTime for some nonstandard evaluation. I will use the rlang package, although in principle we could use functions in base R to accomplish these goals.\nFirst, we are going to use rlang::expr() to capture/quote/defuse the R code as an expression. We can print the code as code, print it as text, and use eval() to show the plot.\n\np_code &lt;- rlang::expr(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = \"white\") +\n    labs(title = \"A basic histogram\")\n)\n\n# print the expressions\np_code\n#&gt; ggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 20, color = \"white\") + \n#&gt;     labs(title = \"A basic histogram\")\n\n# expression =&gt; text\nrlang::expr_text(p_code)\n#&gt; [1] \"ggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 20, color = \\\"white\\\") + \\n    labs(title = \\\"A basic histogram\\\")\"\n\neval(p_code)\n\n\n\n\nThen, it should be straightforward to make the self-documenting plot, right?\n\np_code &lt;- rlang::expr(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = \"white\") +\n    labs(title = \"A basic histogram\")\n)\n\nwrap_elements(eval(p_code)) + \n  plot_annotation(title = rlang::expr_text(p_code), theme = title_theme)  \n\n\n\n\nHey, it reformatted the title! Indeed, in the process of capturing the code, the code formatting was lost. To get something closer to the source code we provided, we have to reformat the captured code before we print it.\nThe styler package provides a suite of functions for reformatting code. We can define our own coding styles/formatting rules to customize how styler works. I like the styler rules used by Garrick Aden-Buie in his grkstyle package, so I will use grkstyle::grk_style_text() to reformat the code.\n\np_code &lt;- rlang::expr(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = \"white\") +\n    labs(title = \"A basic histogram\")\n)\n\nwrap_elements(eval(p_code)) + \n  plot_annotation(\n    title = rlang::expr_text(p_code) |&gt; \n      grkstyle::grk_style_text() |&gt; \n      # reformatting returns a vector of lines,\n      # so we have to combine them\n      paste0(collapse = \"\\n\"), \n    theme = title_theme\n  )"
  },
  {
    "objectID": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#putting-it-all-together",
    "href": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#putting-it-all-together",
    "title": "Self-documenting plots in ggplot2",
    "section": "Putting it all together",
    "text": "Putting it all together\nWhen we write our self_document() function, the only change we have to make is using rlang::enexpr() instead rlang::expr(). The en-variant is used when we want to en-quote exactly what the user provided. Aside from that change, our self_document() function just bundles together all of the code we developed above:\n\nself_document &lt;- function(expr) {\n  monofont &lt;- ifelse(\n    extrafont::choose_font(\"Consolas\") == \"\", \n    \"mono\", \n    \"Consolas\"\n  )\n  \n  p &lt;- rlang::enexpr(expr)\n  title &lt;- rlang::expr_text(p) |&gt; \n    grkstyle::grk_style_text() |&gt; \n    paste0(collapse = \"\\n\")\n  \n  patchwork::wrap_elements(eval(p)) + \n    patchwork::plot_annotation(\n      title = title, \n      theme = theme(\n        plot.title = element_text(\n          family = monofont, hjust = 0, size = rel(.9), \n          margin = margin(0, 0, 5.5, 0, unit = \"pt\")\n        )\n      )\n    )\n}\n\nAnd let’s confirm that it works.\n\nlibrary(ggplot2)\nself_document(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = \"white\") +\n    labs(title = \"A basic histogram\")\n)\n\n\n\n\nBecause we developed this function on top of rlang, we can do some tricks like injecting a variable’s value when capturing the code. For example, here I use !! color to replace the color variable with the actual value.\n\ncolor &lt;- \"white\"\nself_document(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = !! color) +\n    labs(title = \"A basic histogram\")\n)\n\n\n\n\nAnd if you are wondering, yes, we can self_document() a self_document() plot.\n\nself_document(\n  self_document(\n    ggplot(mtcars, aes(x = mpg)) +\n      geom_histogram(bins = 20, color = \"white\") +\n      labs(title = \"A basic histogram\")\n  )\n)"
  },
  {
    "objectID": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#alas-comments-are-lost",
    "href": "posts/2022-03-10-self-titled-ggplot2-plots/index.html#alas-comments-are-lost",
    "title": "Self-documenting plots in ggplot2",
    "section": "Alas, comments are lost",
    "text": "Alas, comments are lost\nOne downside of this approach is that helpful comments are lost.\n\nself_document(\n  ggplot(mtcars, aes(x = mpg)) +\n    geom_histogram(bins = 20, color = !! color) +\n    # get rid of that grey\n    theme_minimal() +\n    labs(title = \"A basic histogram\")\n)\n\n\n\n\nI am not sure how to include comments. One place where comments are stored and printed is in function bodies:\n\nf &lt;- function() {\nggplot(mtcars, aes(x = mpg)) +\n  geom_histogram(bins = 20, color = !! color) +\n  # get rid of that grey\n  theme_minimal() +\n  labs(title = \"A basic histogram\")\n}\n\nprint(f, useSource = TRUE)\n#&gt; function() {\n#&gt; ggplot(mtcars, aes(x = mpg)) +\n#&gt;   geom_histogram(bins = 20, color = !! color) +\n#&gt;   # get rid of that grey\n#&gt;   theme_minimal() +\n#&gt;   labs(title = \"A basic histogram\")\n#&gt; }\n\nI have no idea how to go about exploiting this feature for self-documenting plots, however.\n\n\n\n\n\n\n\nSession info\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21 ucrt)\n os       Windows 11 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-28\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.3.0)\n extrafont     0.19    2023-01-18 [1] CRAN (R 4.3.0)\n extrafontdb   1.0     2012-06-11 [1] CRAN (R 4.3.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.0)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.3.0)\n grkstyle      0.2.1   2023-04-24 [1] Github (gadenbuie/grkstyle@2dac4d7)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.0)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43    2023-05-25 [1] CRAN (R 4.3.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n patchwork   * 1.1.2   2022-08-19 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.0)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R.cache       0.16.0  2022-07-21 [1] CRAN (R 4.3.0)\n R.methodsS3   1.8.2   2022-06-13 [1] CRAN (R 4.3.0)\n R.oo          1.25.0  2022-06-12 [1] CRAN (R 4.3.0)\n R.utils       2.12.2  2022-11-11 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.0)\n ragg          1.2.5   2023-01-12 [1] CRAN (R 4.3.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.3.0)\n rprojroot     2.0.3   2022-04-02 [1] CRAN (R 4.3.0)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n Rttf2pt1      1.3.12  2023-01-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n styler        1.10.1  2023-06-05 [1] CRAN (R 4.3.1)\n systemfonts   1.0.4   2022-02-11 [1] CRAN (R 4.3.0)\n textshaping   0.3.6   2021-10-13 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.0)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n withr         2.5.0   2022-03-03 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.0/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "",
    "text": "Mattan S. Ben-Shachar wrote an excellent tutorial about how to impose ordering constraints in Bayesian regression models. In that post, the data comes from archaeology (inspired by Buck, 2017 but not an exact copy). We have samples from different layers (Layer) in a site, and for each sample, we have a C14 radiocarbon date measurement and its associated measurement error.\nlibrary(tidyverse)\n\ntable1 &lt;- tribble(\n  ~Layer,  ~C14, ~error,\n     \"B\", -5773,     30,\n     \"B\", -5654,     30,\n     \"B\", -5585,     30,\n     \"C\", -5861,     30,\n     \"C\", -5755,     30,\n     \"E\", -5850,     50,\n     \"E\", -5928,     50,\n     \"E\", -5905,     50,\n     \"G\", -6034,     30,\n     \"G\", -6184,     30,\n     \"I\", -6248,     50,\n     \"I\", -6350,     50\n  )\ntable1$Layer &lt;- factor(table1$Layer)\nBecause of how the layers are ordered—new stuff piled on top of older stuff—we a priori expect deeper layers to have older dates, so these are the ordering constraints:\n\\[\n\\mu_{\\text{Layer I}} &lt; \\mu_{\\text{Layer G}} &lt; \\mu_{\\text{Layer E}} &lt; \\mu_{\\text{Layer C}} &lt; \\mu_{\\text{Layer B}}\n\\]\nwhere μ is the average C14 age of a layer.\nBen-Shachar’s post works through some ways in brms to achieve this constraint:\nIn this post, I am going to add another option to this list:"
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html#big-idea-of-contrast-coding",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html#big-idea-of-contrast-coding",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Big idea of contrast coding",
    "text": "Big idea of contrast coding\nWhen our model includes categorical variables, we need some way to code those variables in our model (that is, use numbers to represent the category levels). Our choice of coding scheme will change the meaning of the model parameters, allowing us to perform different comparisons (test different statistical hypotheses) about the means of the category levels. Let’s spell that out again, because it is the big idea of the contrast coding:\ndifferent contrast coding schemes &lt;-&gt; \n  different parameter meanings &lt;-&gt; \n    different comparisons / hypotheses\n(Isn’t that an eye-popping graphic?)\nThe toolbox of contrast coding schemes is deep but also confusing. Whenever I step away from R’s default contrast coding, I usually have these pages open to help me: some tutorial on a UCLA page, Lisa DeBruine’s comparison article, and the menu of contrast schemes in emmeans. So, let’s review the basics by looking at R’s default contrast coding scheme."
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html#the-default-dummy-coding",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html#the-default-dummy-coding",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "The default: dummy coding",
    "text": "The default: dummy coding\nBy default, R will code categorical variables in a regression model using “treatment” or “dummy” coding. In this scheme,\n\nThe intercept is the mean of one of the category levels (the reference level)\nParameters estimate the difference between each other level and the reference level\n\nLet’s fit a simple linear model and work through the parameter meanings:\n\nm1 &lt;- lm(C14 ~ 1 + Layer, table1)\ncoef(m1)\n#&gt; (Intercept)      LayerC      LayerE      LayerG      LayerI \n#&gt;  -5670.6667   -137.3333   -223.6667   -438.3333   -628.3333\n\nHere, the (Intercept) is the mean of the reference level, and the reference level is the level of the categorical variable not listed in the other parameter names (LayerB). Each of the other parameters is a difference from that reference level. Layer C’s mean is (Intercept) + LayerC. The model.matrix() shows how these categorical variables are coded in the model’s design/contrast matrix:\n\n# Matrix has 1 row per observation but we just want 1 per category level\nmat_m1 &lt;- m1 |&gt; \n  model.matrix() |&gt;\n  unique()\nmat_m1\n#&gt;    (Intercept) LayerC LayerE LayerG LayerI\n#&gt; 1            1      0      0      0      0\n#&gt; 4            1      1      0      0      0\n#&gt; 6            1      0      1      0      0\n#&gt; 9            1      0      0      1      0\n#&gt; 11           1      0      0      0      1\n\nThe (Intercept) is the model constant, so naturally, it’s switched on (equals 1) for every row. Each of the other columns are indicator variables. layerC turns on for the layer C rows, layerE turns on for layer E rows, and so on.\nMatrix multiplying the contrast matrix by the model coefficients will compute the mean values of each layer.\n\\[\n\\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol{\\beta}\n\\]\nThink of this equation as a contract for a contrast coding scheme: Multiplying the contrast matrix by the model coefficients should give us the means of the category levels.\n\nmat_m1 %*% coef(m1)\n#&gt;         [,1]\n#&gt; 1  -5670.667\n#&gt; 4  -5808.000\n#&gt; 6  -5894.333\n#&gt; 9  -6109.000\n#&gt; 11 -6299.000\n\n# Means by hand\naggregate(C14 ~ Layer, table1, mean)\n#&gt;   Layer       C14\n#&gt; 1     B -5670.667\n#&gt; 2     C -5808.000\n#&gt; 3     E -5894.333\n#&gt; 4     G -6109.000\n#&gt; 5     I -6299.000\n\nIf the matrix multiplication is too quick, here it is in slow motion where each row has been weighted (multiplied) by coefficients:\n\n# Sums of the rows are the means\nmat_m1 %*% diag(coef(m1))\n#&gt;         [,1]      [,2]      [,3]      [,4]      [,5]\n#&gt; 1  -5670.667    0.0000    0.0000    0.0000    0.0000\n#&gt; 4  -5670.667 -137.3333    0.0000    0.0000    0.0000\n#&gt; 6  -5670.667    0.0000 -223.6667    0.0000    0.0000\n#&gt; 9  -5670.667    0.0000    0.0000 -438.3333    0.0000\n#&gt; 11 -5670.667    0.0000    0.0000    0.0000 -628.3333\n\nSuccessive differences coding\nNow, let’s look at a different kind of coding: (reverse) successive differences coding. In this scheme:\n\nThe intercept is the mean of the levels means\nParameters estimate the difference between adjacent levels\nbut I have to reverse how the levels are ordered in the underlying factor() so that the differences are positive, comparing each layer with the one below it. (LayerB - LayerC should be positive).\n\nWe apply this coding by creating a new factor and setting the contrast(). R lets us set the contrast to the name of a function that computes contrasts, so we use \"contr.sdif\".\n\ncontr.sdif &lt;- MASS::contr.sdif\n\n# Reverse the factor levels\ntable1$LayerAlt &lt;- factor(table1$Layer, rev(levels(table1$Layer)))\n\ncontrasts(table1$LayerAlt) &lt;- \"contr.sdif\"\n\nThen we just fit the model as usual. As intended, the model’s coefficients are different.\n\nm2 &lt;- lm(C14 ~ 1 + LayerAlt, table1)\ncoef(m2)\n#&gt; (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C \n#&gt; -5956.20000   190.00000   214.66667    86.33333   137.33333\n\nWe can compute the mean of layer means and the layer differences by hand to confirm that the model parameters are computing what we expect.\n\n# Make a list so we can write out the diffs easily\nlayer_means &lt;- table1 |&gt; \n  split(~ Layer) |&gt; \n  lapply(function(x) mean(x$C14))\nstr(layer_means)\n#&gt; List of 5\n#&gt;  $ B: num -5671\n#&gt;  $ C: num -5808\n#&gt;  $ E: num -5894\n#&gt;  $ G: num -6109\n#&gt;  $ I: num -6299\n\ndata.frame(\n  model_coef = coef(m2),\n  by_hand = c(\n    mean(unlist(layer_means)),\n    layer_means$G - layer_means$I,\n    layer_means$E - layer_means$G,\n    layer_means$C - layer_means$E,\n    layer_means$B - layer_means$C\n  )\n)\n#&gt;              model_coef     by_hand\n#&gt; (Intercept) -5956.20000 -5956.20000\n#&gt; LayerAltG-I   190.00000   190.00000\n#&gt; LayerAltE-G   214.66667   214.66667\n#&gt; LayerAltC-E    86.33333    86.33333\n#&gt; LayerAltB-C   137.33333   137.33333\n\nBack to our contrast coding contract, we see that the contrast matrix matrix-multiplied by the model coefficients gives us the level means.\n\nmat_m2 &lt;- unique(model.matrix(m2))\n\nmat_m2 %*% coef(m2)\n#&gt;         [,1]\n#&gt; 1  -5670.667\n#&gt; 4  -5808.000\n#&gt; 6  -5894.333\n#&gt; 9  -6109.000\n#&gt; 11 -6299.000\n\n# By hand\naggregate(C14 ~ Layer, table1, mean)\n#&gt;   Layer       C14\n#&gt; 1     B -5670.667\n#&gt; 2     C -5808.000\n#&gt; 3     E -5894.333\n#&gt; 4     G -6109.000\n#&gt; 5     I -6299.000\n\nIt’s so clean and simple. We still get the level means and the parameters estimate specific comparisons of interest to us. So, how are the categorical variables and their differences coded in the model’s contrast matrix?\n\nmat_m2\n#&gt;    (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C\n#&gt; 1            1         0.2         0.4         0.6         0.8\n#&gt; 4            1         0.2         0.4         0.6        -0.2\n#&gt; 6            1         0.2         0.4        -0.4        -0.2\n#&gt; 9            1         0.2        -0.6        -0.4        -0.2\n#&gt; 11           1        -0.8        -0.6        -0.4        -0.2\n\nWait… what? 😕"
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html#the-comparison-matrix",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html#the-comparison-matrix",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "The Comparison Matrix",
    "text": "The Comparison Matrix\nWhen I first started drafting this post, I made it to this point and noped out for a few days. My curiosity did win out eventually, and I hit the books (remembered this tweet and this handout, watched this video, read this paper, and read section 9.1.2 in Applied Regression Analysis & Generalized Linear Models). Now, for the rest of the post.\nThe best formal, citable source for what I describe here is Schad and colleagues (2020), but what they call a “hypothesis matrix”, I’m calling a comparison matrix. I do this for two reasons: 1) to get away from hypothesis testing mindset (see Figure 1) and 2) because we are using the hypothesis matrix to apply a constraint among parameter values (remember that?).\n{% include figure image_path=“2023-07-bayes-sign.jpeg” alt=“In this house, we beleive: Bayes is good, estimate with uncertainty is better than hypothesis testing, math is hard, sampling is easy, Bayesian estimation wtih informative priors is indistinguishable from data falsifications, and it kicks ass.” caption=“Figure 1. The sign in my yard.” %}{: style=“max-width: 66%; display: block; margin: 2em auto;”}\nIn this approach, we define the model parameters β by matrix-multiplying the the comparison matrix C (which activates or weights different level means) and the levels means μ.\n\\[\n\\mathbf{C}\\boldsymbol{\\mu} = \\boldsymbol{\\beta} \\\\\n\\begin{bmatrix}\n  \\textrm{weights for comparison 1} \\\\\n  \\textrm{weights for comparison 2} \\\\\n  \\textrm{weights for comparison 3} \\\\\n  \\cdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\mu_1 \\\\\n  \\mu_2 \\\\\n  \\mu_3 \\\\\n  \\cdots \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\beta_2 \\\\\n  \\cdots \\\\\n\\end{bmatrix}\n\\]\nSo, in the dummy-coded version of the model, we had the following comparison matrix:\n\\[\n\\mathbf{C}_\\text{dummy}\\boldsymbol{\\mu} = \\boldsymbol{\\beta}_\\text{dummy} \\\\\n\\begin{bmatrix}\n  1 & 0 & 0 & 0 & 0 \\\\\n  -1 & 1 & 0 & 0 & 0 \\\\\n  -1 & 0 & 1 & 0 & 0 \\\\\n  -1 & 0 & 0 & 1 & 0 \\\\\n  -1 & 0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\mu_{\\text{Layer B}} \\\\\n  \\mu_{\\text{Layer C}} \\\\\n  \\mu_{\\text{Layer E}} \\\\\n  \\mu_{\\text{Layer G}} \\\\\n  \\mu_{\\text{Layer I}} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\beta_0: \\mu_{\\text{Layer B}} \\\\\n  \\beta_1: \\mu_{\\text{Layer C}} - \\mu_{\\text{Layer B}} \\\\\n  \\beta_2: \\mu_{\\text{Layer E}} - \\mu_{\\text{Layer B}} \\\\\n  \\beta_3: \\mu_{\\text{Layer G}} - \\mu_{\\text{Layer B}} \\\\\n  \\beta_4: \\mu_{\\text{Layer I}} - \\mu_{\\text{Layer B}} \\\\\n\\end{bmatrix}\n\\]\nThe first row in C sets the Layer B as the reference value for the dummy coding. The second row turns on both Layer B and Layer C, but Layer B is negatively weighted. Thus, the corresponding model coefficient is the difference between Layers C and B.\nThe comparison matrix for the reverse successive difference contrast coding is similar. The first row activates all of the layers buts equally weights them, so we get a mean of means for the model intercept. Each row after the first is the difference between two layer means.\n\\[\n\\mathbf{C}_\\text{rev-diffs}\\boldsymbol{\\mu} = \\boldsymbol{\\beta}_\\text{rev-diffs} \\\\\n\\begin{bmatrix}\n  .2 & .2 & .2 & .2 & .2 \\\\\n  0 &  0 &  0 &  1 & -1 \\\\\n  0 &  0 &  1 & -1 &  0 \\\\\n  0 &  1 & -1 &  0 &  0 \\\\\n  1 & -1 &  0 &  0 &  0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\mu_{\\text{Layer B}} \\\\\n  \\mu_{\\text{Layer C}} \\\\\n  \\mu_{\\text{Layer E}} \\\\\n  \\mu_{\\text{Layer G}} \\\\\n  \\mu_{\\text{Layer I}} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\beta_0: \\text{mean of } \\mu \\\\\n  \\beta_1: \\mu_{\\text{Layer G}} - \\mu_{\\text{Layer I}} \\\\\n  \\beta_2: \\mu_{\\text{Layer E}} - \\mu_{\\text{Layer G}} \\\\\n  \\beta_3: \\mu_{\\text{Layer C}} - \\mu_{\\text{Layer E}} \\\\\n  \\beta_4: \\mu_{\\text{Layer B}} - \\mu_{\\text{Layer C}} \\\\\n\\end{bmatrix}\n\\]\nNow, here is the magic part 🔮. Multiplying both sides by the inverse of the comparison matrix will set up a design matrix for the linear model which follows the contract for the contrast matrices I described above:\n\\[\n\\mathbf{C}\\boldsymbol{\\mu} = \\boldsymbol{\\beta} \\\\\n\\mathbf{C}^{-1}\\mathbf{C}\\boldsymbol{\\mu} = \\mathbf{C}^{-1}\\boldsymbol{\\beta} \\\\\n\\boldsymbol{\\mu} = \\mathbf{C}^{-1}\\boldsymbol{\\beta} \\\\\n\\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol{\\beta} \\\\\n\\]\nSo, we can invert1 our comparison matrix to get the model’s contrast matrix:\n\ncomparisons &lt;- c(\n  .2, .2, .2, .2, .2,\n   0,  0,  0,  1, -1,\n   0,  0,  1, -1,  0,\n   0,  1, -1,  0,  0,\n   1, -1,  0,  0,  0\n)\n\nmat_comparisons &lt;- matrix(comparisons, nrow = 5, byrow = TRUE)\nsolve(mat_comparisons)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1  0.2  0.4  0.6  0.8\n#&gt; [2,]    1  0.2  0.4  0.6 -0.2\n#&gt; [3,]    1  0.2  0.4 -0.4 -0.2\n#&gt; [4,]    1  0.2 -0.6 -0.4 -0.2\n#&gt; [5,]    1 -0.8 -0.6 -0.4 -0.2\n\nmat_m2\n#&gt;    (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C\n#&gt; 1            1         0.2         0.4         0.6         0.8\n#&gt; 4            1         0.2         0.4         0.6        -0.2\n#&gt; 6            1         0.2         0.4        -0.4        -0.2\n#&gt; 9            1         0.2        -0.6        -0.4        -0.2\n#&gt; 11           1        -0.8        -0.6        -0.4        -0.2\n\nOr, perhaps more commonly, we can take the contrast matrix used by a model and recover the comparison matrix, which is a nice trick when we have R automatically set the contrast values for us:\n\n# Dummy coding example\nmat_m1\n#&gt;    (Intercept) LayerC LayerE LayerG LayerI\n#&gt; 1            1      0      0      0      0\n#&gt; 4            1      1      0      0      0\n#&gt; 6            1      0      1      0      0\n#&gt; 9            1      0      0      1      0\n#&gt; 11           1      0      0      0      1\nsolve(mat_m1)\n#&gt;              1 4 6 9 11\n#&gt; (Intercept)  1 0 0 0  0\n#&gt; LayerC      -1 1 0 0  0\n#&gt; LayerE      -1 0 1 0  0\n#&gt; LayerG      -1 0 0 1  0\n#&gt; LayerI      -1 0 0 0  1\n\n# Successive differences coding example\nmat_m2\n#&gt;    (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C\n#&gt; 1            1         0.2         0.4         0.6         0.8\n#&gt; 4            1         0.2         0.4         0.6        -0.2\n#&gt; 6            1         0.2         0.4        -0.4        -0.2\n#&gt; 9            1         0.2        -0.6        -0.4        -0.2\n#&gt; 11           1        -0.8        -0.6        -0.4        -0.2\nsolve(mat_m2)\n#&gt;               1    4    6    9   11\n#&gt; (Intercept) 0.2  0.2  0.2  0.2  0.2\n#&gt; LayerAltG-I 0.0  0.0  0.0  1.0 -1.0\n#&gt; LayerAltE-G 0.0  0.0  1.0 -1.0  0.0\n#&gt; LayerAltC-E 0.0  1.0 -1.0  0.0  0.0\n#&gt; LayerAltB-C 1.0 -1.0  0.0  0.0  0.0\n\nAs I said earlier, there are all kinds of contrast coding schemes which allow us to define the model parameters in terms of specific comparisons, and this post only mentions two such schemes (dummy coding and a reversed version of successive differences coding)."
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html#finally-in-layer-i-of-this-post-the-brms-model",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html#finally-in-layer-i-of-this-post-the-brms-model",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Finally, in Layer I of this post, the brms model",
    "text": "Finally, in Layer I of this post, the brms model\nNow that we know about contrasts, and how they let us define model parameters in terms of the comparisons we want to make, we can use this technique to enforce an ordering constraint.\nWe set up our model as in Ben-Shachar’s post, but here we set a prior for normal(500, 250) on the non-intercept coefficients with a lower-bound of 0 lb = 0 to enforce the ordering constraint.\n\nlibrary(brms)\npriors &lt;- \n  set_prior(\"normal(-5975, 1000)\", class = \"Intercept\") + \n  set_prior(\"normal(500, 250)\", class = \"b\", lb = 0) +\n  set_prior(\"exponential(0.01)\", class = \"sigma\")\n\nvalidate_prior(\n  priors,\n  bf(C14 | se(error, sigma = TRUE) ~ 1 + LayerAlt),\n  data = table1\n)\n#&gt;                prior     class        coef group resp dpar nlpar lb ub\n#&gt;     normal(500, 250)         b                                    0   \n#&gt;     normal(500, 250)         b LayerAltBMC                        0   \n#&gt;     normal(500, 250)         b LayerAltCME                        0   \n#&gt;     normal(500, 250)         b LayerAltEMG                        0   \n#&gt;     normal(500, 250)         b LayerAltGMI                        0   \n#&gt;  normal(-5975, 1000) Intercept                                        \n#&gt;    exponential(0.01)     sigma                                    0   \n#&gt;        source\n#&gt;          user\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;          user\n#&gt;          user\n\nWe fit the model:\n\nm3 &lt;- brm(\n  bf(C14 | se(error, sigma = TRUE) ~ 1 + LayerAlt),\n  family = gaussian(\"identity\"),\n  prior = priors,\n  data = table1,\n  seed = 4321,\n  backend = \"cmdstanr\",\n  cores = 4, \n  # caching\n  file = \"2023-07-03\", \n  file_refit = \"on_change\"\n)\n\nWe can see that the level differences are indeed positive with 95% intervals of positive values.\n\nsummary(m3)\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: C14 | se(error, sigma = TRUE) ~ 1 + LayerAlt \n#&gt;    Data: table1 (Number of observations: 12) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept   -5957.60     27.91 -6011.89 -5900.71 1.00     1964     1715\n#&gt; LayerAltGMI   211.00     82.29    51.67   378.86 1.00     1693      939\n#&gt; LayerAltEMG   206.15     71.30    68.47   349.07 1.00     1937     1185\n#&gt; LayerAltCME   105.55     62.84     7.90   243.81 1.00     1377     1023\n#&gt; LayerAltBMC   145.95     65.13    23.63   279.12 1.00     1684      857\n#&gt; \n#&gt; Family Specific Parameters: \n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma    79.03     26.95    41.05   142.49 1.00     1651     2149\n#&gt; \n#&gt; Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).\nbayesplot::mcmc_intervals(m3, regex_pars = \"Layer\")\n\n\n\nEstimates of the level differences.\n\n\n\n\nconditional_effects(m3)\n\n\n\nConditional means for each layer."
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html#normally-i-dont-think-you-need-contrast-codes",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html#normally-i-dont-think-you-need-contrast-codes",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Normally, I don’t think you need contrast codes",
    "text": "Normally, I don’t think you need contrast codes\nMy general advice for contrast coding is to just fit the model and then have the software compute the appropriate estimates and comparisons afterwards on the outcome scale. For example, emmeans can take a fitted model, run requested comparisons, and handle multiple comparisons and p-value adjustments for us. marginaleffects probably does this too. (I really need to play with it.) And in a Bayesian model, we can compute comparisons of interest by doing math on the posterior samples (estimating things and computing differences and summarizing the distribution of the differences), but this particular model, where the coding was needed to impose the prior ordering constraint, ruled out the posterior post-processing approach.\n\nSession info:\n\n.session_info\n#&gt; ─ Session info ───────────────────────────────────────────────────────────────\n#&gt;  setting         value\n#&gt;  version         R version 4.3.0 (2023-04-21 ucrt)\n#&gt;  os              Windows 11 x64 (build 22621)\n#&gt;  system          x86_64, mingw32\n#&gt;  ui              RTerm\n#&gt;  language        (EN)\n#&gt;  collate         English_United States.utf8\n#&gt;  ctype           English_United States.utf8\n#&gt;  tz              America/Chicago\n#&gt;  date            2023-07-27\n#&gt;  pandoc          3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt;  stan (rstan)    2.26.1\n#&gt;  stan (cmdstanr) 2.32.0\n#&gt; \n#&gt; ─ Packages ───────────────────────────────────────────────────────────────────\n#&gt;  ! package        * version date (UTC) lib source\n#&gt;    abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.0)\n#&gt;    backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n#&gt;    base64enc        0.1-3   2015-07-28 [1] CRAN (R 4.3.0)\n#&gt;    bayesplot        1.10.0  2022-11-16 [1] CRAN (R 4.3.0)\n#&gt;    bridgesampling   1.1-2   2021-04-16 [1] CRAN (R 4.3.0)\n#&gt;    brms           * 2.19.0  2023-03-14 [1] CRAN (R 4.3.0)\n#&gt;    Brobdingnag      1.2-9   2022-10-19 [1] CRAN (R 4.3.0)\n#&gt;    callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.0)\n#&gt;    checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.0)\n#&gt;    cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;    cmdstanr         0.5.3   2023-04-24 [1] github (stan-dev/cmdstanr)\n#&gt;    coda             0.19-4  2020-09-30 [1] CRAN (R 4.3.0)\n#&gt;    codetools        0.2-19  2023-02-01 [2] CRAN (R 4.3.0)\n#&gt;    colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;    colourpicker     1.2.0   2022-10-28 [1] CRAN (R 4.3.0)\n#&gt;    crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.0)\n#&gt;    crosstalk        1.2.0   2021-11-04 [1] CRAN (R 4.3.0)\n#&gt;    curl             5.0.1   2023-06-07 [1] CRAN (R 4.3.0)\n#&gt;    digest           0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;    distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;    dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;    DT               0.28    2023-05-18 [1] CRAN (R 4.3.0)\n#&gt;    dygraphs         1.1.1.6 2018-07-11 [1] CRAN (R 4.3.0)\n#&gt;    ellipsis         0.3.2   2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;    emmeans          1.8.7   2023-06-23 [1] CRAN (R 4.3.1)\n#&gt;    estimability     1.4.1   2022-08-05 [1] CRAN (R 4.3.0)\n#&gt;    evaluate         0.21    2023-05-05 [1] CRAN (R 4.3.0)\n#&gt;    fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.0)\n#&gt;    farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n#&gt;    fastmap          1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;    forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n#&gt;    generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;    ggplot2        * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n#&gt;    glue             1.6.2   2022-02-24 [1] CRAN (R 4.3.0)\n#&gt;    gridExtra        2.3     2017-09-09 [1] CRAN (R 4.3.0)\n#&gt;    gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;    gtools           3.9.4   2022-11-27 [1] CRAN (R 4.3.0)\n#&gt;    hms              1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;    htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;    htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.0)\n#&gt;    httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.0)\n#&gt;    igraph           1.5.0.1 2023-07-23 [1] CRAN (R 4.3.1)\n#&gt;    inline           0.3.19  2021-05-31 [1] CRAN (R 4.3.0)\n#&gt;    jsonlite         1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n#&gt;    knitr            1.43    2023-05-25 [1] CRAN (R 4.3.0)\n#&gt;    labeling         0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n#&gt;    later            1.3.1   2023-05-02 [1] CRAN (R 4.3.0)\n#&gt;    lattice          0.21-8  2023-04-05 [2] CRAN (R 4.3.0)\n#&gt;    lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n#&gt;    loo              2.6.0   2023-03-31 [1] CRAN (R 4.3.0)\n#&gt;    lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;    magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;    markdown         1.7     2023-05-16 [1] CRAN (R 4.3.0)\n#&gt;    MASS           * 7.3-60  2023-05-04 [1] CRAN (R 4.3.0)\n#&gt;    Matrix           1.6-0   2023-07-08 [1] CRAN (R 4.3.1)\n#&gt;    matrixStats      1.0.0   2023-06-02 [1] CRAN (R 4.3.0)\n#&gt;    mime             0.12    2021-09-28 [1] CRAN (R 4.3.0)\n#&gt;    miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.0)\n#&gt;    munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n#&gt;    mvtnorm          1.2-2   2023-06-08 [1] CRAN (R 4.3.1)\n#&gt;    nlme             3.1-162 2023-01-31 [2] CRAN (R 4.3.0)\n#&gt;    pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;    pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n#&gt;    pkgconfig        2.0.3   2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;    plyr             1.8.8   2022-11-11 [1] CRAN (R 4.3.0)\n#&gt;    posterior        1.4.1   2023-03-14 [1] CRAN (R 4.3.0)\n#&gt;    prettyunits      1.1.1   2020-01-24 [1] CRAN (R 4.3.0)\n#&gt;    processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n#&gt;    promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.0)\n#&gt;    ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.0)\n#&gt;    purrr          * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n#&gt;    R6               2.5.1   2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;    ragg             1.2.5   2023-01-12 [1] CRAN (R 4.3.0)\n#&gt;    Rcpp           * 1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;  D RcppParallel     5.1.7   2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;    readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;    reshape2         1.4.4   2020-04-09 [1] CRAN (R 4.3.0)\n#&gt;    rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n#&gt;    rmarkdown        2.23    2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;    rstan            2.26.22 2023-05-02 [1] local\n#&gt;    rstantools       2.3.1.1 2023-07-18 [1] CRAN (R 4.3.1)\n#&gt;    rstudioapi       0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;    scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n#&gt;    sessioninfo      1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n#&gt;    shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;    shinyjs          2.1.0   2021-12-23 [1] CRAN (R 4.3.0)\n#&gt;    shinystan        2.6.0   2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;    shinythemes      1.2.0   2021-01-25 [1] CRAN (R 4.3.0)\n#&gt;    StanHeaders      2.26.27 2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;    stringi          1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;    stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n#&gt;    systemfonts      1.0.4   2022-02-11 [1] CRAN (R 4.3.0)\n#&gt;    tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.0)\n#&gt;    textshaping      0.3.6   2021-10-13 [1] CRAN (R 4.3.0)\n#&gt;    threejs          0.3.3   2020-01-21 [1] CRAN (R 4.3.0)\n#&gt;    tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;    tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n#&gt;    tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;    tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n#&gt;    timechange       0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;    tzdb             0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n#&gt;    utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.0)\n#&gt;    V8               4.3.3   2023-07-18 [1] CRAN (R 4.3.1)\n#&gt;    vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;    withr            2.5.0   2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;    xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;    xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.0)\n#&gt;    xts              0.13.1  2023-04-16 [1] CRAN (R 4.3.0)\n#&gt;    yaml             2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;    zoo              1.8-12  2023-04-13 [1] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n#&gt;  [2] C:/Program Files/R/R-4.3.0/library\n#&gt; \n#&gt;  D ── DLL MD5 mismatch, broken installation.\n#&gt; \n#&gt; ──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-07-03-bayesian-ordering-constraint/index.html#footnotes",
    "href": "posts/2023-07-03-bayesian-ordering-constraint/index.html#footnotes",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Footnotes",
    "text": "Footnotes\n\nI use solve() here for the inversion, but Schad and colleagues (2020) use the generalized inverse MASS::ginv() or matlib::Ginv(). solve() only works on square matrices, but the generalized inverse works on non-square matrices.↩︎"
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html",
    "title": "playing with quatro",
    "section": "",
    "text": "This post is a set of notes on how I migrated my blog from Jekyll (a Ruby based blogging system) to Quarto (a Pandoc/RMarkdown-ish based blogging system)."
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#big-ideas-first",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#big-ideas-first",
    "title": "playing with quatro",
    "section": "big ideas first",
    "text": "big ideas first\nOver the years on the Jekyll site, blogging from 2016 to 2023, I had slowly opted into customizations, tricks and workarounds. I developed my own build pipeline using targets and knitr. Each post included a _footer.Rmd child document that included the session information at the bottom of the post. I could never get downlit source-code linking to work with Jekyll’s syntax highlighting system, so I recreated the effect where I could. R and Windows could gobble up emoji until very recently so I had used emo::ji() and Github :inline_emoji: syntax as a workaround. I sometimes used Jekyll-specific syntax and site-specific theming—a no-go in Quarto-land—and I have to port them over now too. All of these features and customizations become potential liabilities at migration time."
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#why-make-the-switch",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#why-make-the-switch",
    "title": "playing with quatro",
    "section": "why make the switch",
    "text": "why make the switch"
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#getting-started",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#getting-started",
    "title": "playing with quatro",
    "section": "getting started",
    "text": "getting started\nQuarto blogs are a supported project template in RStudio, so to create the blog I went to File &gt; New Project &gt; New Directory &gt; Quarto Blog to get the project infrastructure in place. To preview/build the site, I could click Build &gt; Render Website or Ctrl + Shift + B. Easy peasy."
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#file-locations",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#file-locations",
    "title": "playing with quatro",
    "section": "file locations",
    "text": "file locations\nMy Jekyll blog had the following, very simplified, high-level structure.\n.\n├── 📁 _R \n│   ├── 📁 _drafts\n│   ├── 📄 2022-03-10-self-titled-ggplot2-plots.Rmd\n│   └── [other .Rmd files]\n├── 📁 _posts\n│   ├── 📄 2022-03-10-self-titled-ggplot2-plots.md\n│   └── [other .md files]\n├── 📁 _drafts\n├── 📁 _site\n│   ├── 📁 self-titled-ggplot2-plots\n│   │   └── 📄 index.html\n│   ├── [other directories for each post]\n|   └── 📄 index.html\n└── 📁 assets\n|   ├── 📁 css\n|   ├── 📁 images\n|   |   ├── 🌅 2022-03-neon.jpg\n|   |   └── [other images for each post]\n|   └── 📁 js\n└── 📄 _targets.R\nWhere the workflow is:\n\nI write the post as .Rmd file in the _R folder.\nI have targets (_targets.R) check the .Rmd files for changes and run knitr on any that have changed, creating the .md file in the _posts folder. (targets also spell-checks all of the files on each run.)\nI push the .md files to GitHub, and it runs Jekyll to produce the folders and .html files in the _site folder and serve the _site folder to visitors.\n\nIf I want to include any non-figure images, I have to store them in assets/images. If want to develop a draft, I have to keep it in a separate workflow. (If an .md file ends up in _posts, it is published.) Finally, the date in the filename is significant. It sets the “date published” for the post, and the date is later stripped off for the final address of the post in the published site.\nHere is the parallel set up in my Quarto site:\n.\n├── 📁 posts \n│   ├── 📁 2022-03-10-self-titled-ggplot2-plots\n│   |   ├── 🌅 2022-03-neon.jpg\n│   |   └── 📄 index.qmd\n│   ├── [other directories for each post]\n│   └── 📄 _metadata.yml\n├── 📁 docs\n│   ├── 📁 posts\n|   │   ├── 📁 2022-03-10-self-titled-ggplot2-plots\n|   │   |   ├── 🌅 2022-03-neon.jpg\n|   │   |   └── 📄 index.html\n|   |   └── [other directories for each post]\n│   └── 📄 index.html\n└── 📄 _quarto.yml\nHere, Quarto runs the .qmd -&gt; .md -&gt; .html pipeline, and the docs folder is published as-is as the website on GitHub.\nLook at how much simpler this is layout is. Notably:\n\nThe _drafts folder business is gone, because we use document metadata data (set draft: true in the YAML header) to indicate draft status.\nAssets for individual posts are stored alongside the post in the post directory.\n\nHere, the date in the filename is not significant to Quarto; it helps me out because I want my posts to be sorted in chronological order by date published. In order to set the date for a post, I have to set date: \"2023-07-03\" in the document’s YAML header.\nbroken URLs and redirects\nThe addresses created by Jekyll did not include the date: self-titled-ggplot2-plots/index.html. But in Quarto, the folder name carries over to the site: posts/2022-03-10-self-titled-ggplot2-plots/index.html. I am not sure how to get Quarto to automatically change output folder names, and this sucks because any pre-existing links to my posts now point to broken URLS.\nFortunately, document metadata can help us here by letting us create an alias for the post:\n\n\n\nQuarto index.qmd front matter (key portion)\n\naliases:\n  - \"/self-titled-ggplot2-plots/\"\n\n\nQuarto will create a simple page at /self-titled-ggplot2-plots/index.html that will redirect to the proper URL."
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#the-bare-minimum-for-migrating-a-post",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#the-bare-minimum-for-migrating-a-post",
    "title": "playing with quatro",
    "section": "the bare minimum for migrating a post",
    "text": "the bare minimum for migrating a post\nTo migrate a single post, we need to do the following steps:\n\nDownload the .Rmd file from _R/[post-date-and-filename].Rmd and save it to posts/[post-date-and-filename]/index.qmd.\nDownload any assets referenced in the .Rmd file to posts/[post-date-and-filename] and update the locations in the .qmd file accordingly.\nMigrate YAML metadata.\nReplace Jekyll-specific syntax and theming with Quarto versions.\n\nFor things like steps 1–3, I was able to write R functions to handle this migration. For things like 4, I wrote an R function to run a series of checks on a file for potential issues.\nHere is an example of the functions running on two posts.\n\nsource(\"import-posts.R\")\nbase_url &lt;- \"https://raw.githubusercontent.com/tjmahr/tjmahr.github.io/20e6ab146716425f485ca4f0c81f18aaccebb10f\" \npost_dir &lt;- \"_demo/posts\"\n\ndata_post &lt;- import_jekyll_post(\n  target_post = \"_R/2022-03-10-self-titled-ggplot2-plots.Rmd\", \n  base_url = base_url, \n  post_dir = post_dir\n)\n#&gt; ✔ Post folder created '_demo/posts/2022-03-10-self-titled-ggplot2-plots'\n#&gt; ✔ Migrated `2022-03-neon.jpg`\n#&gt; ✔ Post file created '_demo/posts/2022-03-10-self-titled-ggplot2-plots/index.qmd'\n\ndata_post$lines_current |&gt; \n  check_post()\n#&gt; Warning: Manually linked code found:\n#&gt; [71] [`element_text()`](`r a_element_text`):\n#&gt; [115] [`wrap_elements()`](`r a_wrap_elements`) to c\n#&gt; [157] [`parse()`](`r a_parse`) and evaluate\n#&gt; [158] [`eval()`](`r a_eval`):\n#&gt; [176] First, we are going to use [`rlang::expr()`](`r a_expr`) to\n#&gt; [244] make is using [`rlang::enexpr()`](`r a_enexpr`) ins\n\ndata_post &lt;- import_jekyll_post(\n  target_post = \"_R/2023-07-03-bayesian-ordering-constraint.Rmd\", \n  base_url = base_url, \n  post_dir = post_dir\n)\n#&gt; ✔ Post folder created '_demo/posts/2023-07-03-bayesian-ordering-constraint'\n#&gt; ✔ Migrated `2023-07-ruins-1280.jpg`\n#&gt; ✔ Migrated `2023-07-bayes-sign.jpeg`\n#&gt; ✔ Post file created '_demo/posts/2023-07-03-bayesian-ordering-constraint/index.qmd'\n\ndata_post$lines_current |&gt; \n  check_post()\n#&gt; Warning: Jekyll macro syntax found:\n#&gt; [289] {% include figure image_path=\"20\n#&gt; Warning: Manually linked code found:\n#&gt; [88] Use brms's monotonic effect [`mo()`][brms-mo] syntax.\n#&gt; [200] [`contrast()`](https://rdrr.io/r/stats/c\n#&gt; [519] [`matlib::Ginv()`](https://cran.r-proj\n\nThe check_post() function runs over a file and looks for the lines of text with the following issues:\n\nreferences to assets/\n\nJekyll syntax\nmanually linked code (which should be handled by downlit)\nemoji workarounds (use native emoji)"
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#specific-migration-problems",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#specific-migration-problems",
    "title": "playing with quatro",
    "section": "specific migration problems",
    "text": "specific migration problems\npost footers\nAt the end of each post on the Jekyll site, I included a footer with\n\nthe date the post was knitted,\na link to the post’s source code on GitHub, and\na footnote with the session info.\n\nWe can get (1) for free by using inline R code in the YAML front matter on each post:\n\n\n\nindex.qmd front matter (key portion)\n\ndate-modified: \"`r format(Sys.Date())`\"\n\n\nWe can also get (2) for free by using more YAML features. First, we tell Quarto the URL for the GitHub repo:\n\n\n\n_quarto.yml (key portion)\n\nwebsite:\n  repo-url: \"https://github.com/tjmahr/quarto-blog\"\n\n\nThen enable links to the source code on all the posts. I probably could have included this setting _quarto.yml but I didn’t want to deal with the link being enabled unintentionally if I create any non-blogpost documents.\n\n\n\nposts/_metadata.yml (key portion)\n\ncode-tools:\n  source: repo\n\n\nAs for (3), I had implemented this featured originally in a pretty unsophisticated manner: A file called _R/_footer.Rmd generated the post footer, and I included it as a child document in knitr at the bottom of every post.\n```{r, include = FALSE}\n.parent_doc &lt;- knitr::current_input()\n```\n```{r, child = \"_R/_footer.Rmd\"}\n```\nWe are going to do the same thing, but using Quarto. The bottom of each post now includes an include shortcode:\n{{&lt; include ../_footer.Rmd &gt;}}\nFile-paths in a blog post are relative to the index.qmd file—which is nice for things like datasets or image assets—so to go up to the parent directory for _footer.Rmd, we use ../.\npost headers\nOn the Jekyll site, I could include a banner image, a caption for the banner image, and a color overlay for the image using YAML metadata:\n\n\n\nJekyll Rmd front matter (key portion)\n\nheader:\n  overlay_image: assets/images/2022-03-neon.jpg\n  image_description: A neon sign that says 'neon'\n  overlay_filter: rgba(10, 10, 10, 0.5)\n  caption: 'Photo credit: [**Slava Kuzminsk**](https://unsplash.com/photos/qnHOFl4VuFc)'\n\n\nHere is the result:\n\n\n\nThe post featured here is about ggplot2 plots that contain their own source code in the title, so the background image of a neon sign of the word “neon” is a thematic gag.\nWe can get a banner image in Quarto using the YAML front matter:\n\n\n\nQuarto index.qmd front matter (key portion)\n\ntitle-block-banner: 2022-03-neon.jpg\nimage: 2022-03-neon.jpg\n\n\nwhere image set the image that accompanies the post on the list of blog posts.\nHere is the result:\n\n\n\nNote that we cannot see the word “neon” and the title is hard to read without the overlaid color on the image. We can fix these issues by using CSS to create a color background that blends with the image and centering the background image. I also set the font-color to be white for added contrast. In Jekyll, we could customize the color layer for each post, but we do not have that ability here. We store this custom css in title-block.css and tell _quarto.yaml about it.\n\n\n\ntitle-block.css\n\n.quarto-title-block .quarto-title-banner {\n  background-position-x: 50%;\n  background-position-y: 50%;\n  background-blend-mode: overlay;\n  background-color: rgba(10, 10, 10, .5);\n  color: rgb(255, 255, 255);\n}\n\n\n\n\n\n\n_quarto.yml (key portion)\n\nformat:\n  html:\n    css:\n      - title-block.css\n\n\n\nWe still have a major problem at this point: We cannot use these banner images without including the photo credit credit. Fortunately, we can patch this title banner by using a template partial. Quarto documents like these blog posts are built by plugging data into an HTML template, and this template is itself templated (these subtemplates are the partials). That means, we can change just the title banner template and keep the other parts in tact.\nFirst, let’s make the photo caption into a structured piece of metadata in the YAML front matter. This step will create pieces of data the template can see:\n\n\n\nindex.qmd front matter (key portion)\n\ntitle-block-banner-caption: \n  credit: Slava Kuzminsk\n  link: https://unsplash.com/photos/qnHOFl4VuFc\n\n\nWe download the HTML title banner template from GitHub and store it in partials/title-block.html. We then modify the file, adding the unindented lines towards the bottom.\n\n\n\npartials/title-block.html\n\n&lt;header id=\"title-block-header\" class=\"quarto-title-block default$if(quarto-template-params.banner-header-class)$ $quarto-template-params.banner-header-class$$endif$\"&gt;\n  &lt;div class=\"quarto-title-banner\"&gt;\n    &lt;div class=\"quarto-title column-body\"&gt;\n      &lt;h1 class=\"title\"&gt;$title$&lt;/h1&gt;\n      $if(subtitle)$\n      &lt;p class=\"subtitle lead\"&gt;$subtitle$&lt;/p&gt;\n      $endif$\n      $if(description)$\n      &lt;div&gt;\n        &lt;div class=\"description\"&gt;\n          $description$\n        &lt;/div&gt;\n      &lt;/div&gt;\n      $endif$\n      $if(categories)$\n        $if(quarto-template-params.title-block-categories)$\n      &lt;div class=\"quarto-categories\"&gt;\n        $for(categories)$\n        &lt;div class=\"quarto-category\"&gt;$categories$&lt;/div&gt;\n        $endfor$\n      &lt;/div&gt;\n        $endif$\n      $endif$\n    &lt;/div&gt;\n\n&lt;!-- 🆕 start --&gt;\n$if(title-block-banner-caption)$\n&lt;div class = \"title-block-caption\"&gt;\nPhoto credit: &lt;a href=\"$title-block-banner-caption.link$\"&gt;$title-block-banner-caption.credit$&lt;/a&gt;\n&lt;/div&gt;\n$endif$\n&lt;!-- 🆕 end --&gt;\n  \n  &lt;/div&gt;\n  $title-metadata.html()$\n&lt;/header&gt;\n\n\nIn Pandoc’s templating language, we can refer to YAML front matter variables by using dollar signs. For example, $title$ inserts the title, and $title-block-banner-caption.link$ inserts the link property of the title-block-banner-caption property. In the added lines, we check for a title-block-banner-caption and then add a &lt;div&gt; with a link to the photo source.\nTo activate this partial template, we have to add a field to _quarto.yml:\n\n\n\n_quarto.yml (key portion)\n\nformat:\n  html:\n    template-partials:\n      - partials/title-block.html\n\n\nWe need this addition to the title block to look okay, so we amend title-block.css:\n\n\n\ntitle-block.css\n\n.quarto-title-block .quarto-title-banner {\n  background-position-x: 50%;\n  background-position-y: 50%;\n  background-blend-mode: overlay;\n  background-color: rgba(10, 10, 10, .5);\n  color: rgb(255, 255, 255);\n}\n\n.quarto-title-block .quarto-title-banner .title-block-caption {\n  font-size: .75em;\n  color: rgba(255,255,255,.7);\n}\n\n.quarto-title-block .quarto-title-banner .title-block-caption a {\n  text-decoration: underline !important;\n  color: rgba(255,255,255,.7);\n}\n\n\n\nAt last, we have a decent looking title banner with a background image and a photo credit."
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#other-notes",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#other-notes",
    "title": "playing with quatro",
    "section": "other notes",
    "text": "other notes\nhomepage / listings\ntktktk\ndownlit links\nMy favorite feature of the Quarto style is the downlit links, where R code is automatically linked to documentation. This piece of code stats::lm() is linked, and I didn’t have to do anything! In my old blog, I would replicate this style by manually adding links or using downlit::autolink() to create links for me. But the links added by downlit and those done by hand have a differently style in the rendered document:\n\n\nstats::lm() linked by downlit\n\nstats::lm() linked manually\n\nThis link inconsistency is annoying, so it became one of my automatic checks.\nWhen I was working on this post, I couldn’t get the above demonstration to work. downlit appeared not to process the file unless the file had an explicit R code block in it which was mystifying!\nBy default, the downlit links are not underlined, so I created a file called downlit-underline.css and added it to _quarto.yml. This css is something I had copy-pasted for use in my rmarkdown notebooks, and I have forgetten where I originally found it. I have included it as a comment.\n\n\n\ndownlit-underline.css\n\ncode a:any-link {\n  color: inherit; /* use colour from syntax highlighting */\n  text-decoration: underline;\n  text-decoration-color: #ccc;\n}\n\n\n\n\n\n\n_quarto.yml (key portion)\n\nformat:\n  html:\n    css:\n      - downlit-underline.css"
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html#scratch",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html#scratch",
    "title": "playing with quatro",
    "section": "scratch",
    "text": "scratch\n\nI want a banner image on top like the old site with a credit to the source.\nI had to put fig-width: 8 under format &gt; html in _quarto.yml. I also pushed other knitr options here:\n\n    fig-asp: 0.618\n    fig-dpi: 300\n    fig-align: \"center\"\n    fig-cap-location: \"margin\"\n\n\nI am not sure how to replicate the out.width knitr option.\n\nI want to use targets instead of freeze.\n\nI need old URLs to redirect. Use aliases: [\"\"] in header.\n\nI got to opt into fig-cap-location: \"margin\".\nI had to remove the old knitr _cached stuff. Still need a different solution.\nI don’t want the green links. If I set the link color, that still leaves the green link color in the table of contents.\n\nI want a slightly larger font size. Set fontsize: 1.1em in _quarto.yml\nAll of my old manual downlit attempts are styled differently than actual downlit urls. Added a check.\nI want underlined downlit links. Done by adding a new css file.\n\nI have to convert fig.alt and fig.cap to here.\n#| fig-alt: \"Examples of `plot_dist()`\"\n#| fig-cap:\nThis is the not first post in a Quarto blog. Welcome! It’s configuration. Lots of configuration.\n\nproject level\ndirectory level\npost-level yaml\n\n\n\n\n\n\n\n\nSession info\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21 ucrt)\n os       Windows 11 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Chicago\n date     2023-07-28\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.353\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n brio          1.1.3   2021-11-30 [1] CRAN (R 4.3.0)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n curl          5.0.1   2023-06-07 [1] CRAN (R 4.3.0)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.3.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.0)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.0)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43    2023-05-25 [1] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.0)\n ragg          1.2.5   2023-01-12 [1] CRAN (R 4.3.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.3.0)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n systemfonts   1.0.4   2022-02-11 [1] CRAN (R 4.3.0)\n textshaping   0.3.6   2021-10-13 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.0)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.0/library\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]