[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TJ Mahr‚Äôs blog",
    "section": "",
    "text": "Builder Teej is yelling something at us! üë∑\n\n\n\nWhoa whoa, you see the barricades here? üößüößüöß\nThis place is under construction! This is just a temporary sandbox while I (figure out how to) migrate to a quarto blog!\nAnd besides I can‚Äôt let you in without a hard hat on.\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOrdering constraints in brms using contrast coding\n\n\n\n\n\nBut mostly how contrast matrices are computed\n\n\n\n\n\n\nJul 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nplaying with quatro\n\n\n\n\n\nthis is STILL in beta\n\n\n\n\n\n\nJul 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRecent adventures with lazyeval\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2016\n\n\n\n\n\n\n  \n\n\n\n\nConfusion matrix statistics on late talker diagnoses\n\n\n\n\n\nPositive predictive values and the like.\n\n\n\n\n\n\nOct 6, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html",
    "href": "posts/bayesian-ordering-constraint/index.html",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "",
    "text": "Mattan S. Ben-Shachar wrote an excellent tutorial about how to impose ordering constraints in Bayesian regression models. In that post, the data comes from archaeology (inspired by Buck, 2017 but not an exact copy). We have samples from different layers (Layer) in a site, and for each sample, we have a C14 radiocarbon date measurement and its associated measurement error.\nlibrary(tidyverse)\n\ntable1 &lt;- tribble(\n  ~Layer,  ~C14, ~error,\n     \"B\", -5773,     30,\n     \"B\", -5654,     30,\n     \"B\", -5585,     30,\n     \"C\", -5861,     30,\n     \"C\", -5755,     30,\n     \"E\", -5850,     50,\n     \"E\", -5928,     50,\n     \"E\", -5905,     50,\n     \"G\", -6034,     30,\n     \"G\", -6184,     30,\n     \"I\", -6248,     50,\n     \"I\", -6350,     50\n  )\ntable1$Layer &lt;- factor(table1$Layer)\nBecause of how the layers are ordered‚Äînew stuff piled on top of older stuff‚Äîwe a priori expect deeper layers to have older dates, so these are the ordering constraints:\n\\[\n\\mu_{\\text{Layer I}} &lt; \\mu_{\\text{Layer G}} &lt; \\mu_{\\text{Layer E}} &lt; \\mu_{\\text{Layer C}} &lt; \\mu_{\\text{Layer B}}\n\\]\nwhere Œº is the average C14 age of a layer.\nBen-Shachar‚Äôs post works through some ways in brms to achieve this constraint:\nIn this post, I am going to add another option to this list:"
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html#big-idea-of-contrast-coding",
    "href": "posts/bayesian-ordering-constraint/index.html#big-idea-of-contrast-coding",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Big idea of contrast coding",
    "text": "Big idea of contrast coding\nWhen our model includes categorical variables, we need some way to code those variables in our model (that is, use numbers to represent the category levels). Our choice of coding scheme will change the meaning of the model parameters, allowing us to perform different comparisons (test different statistical hypotheses) about the means of the category levels. Let‚Äôs spell that out again, because it is the big idea of the contrast coding:\ndifferent contrast coding schemes &lt;-&gt; \n  different parameter meanings &lt;-&gt; \n    different comparisons / hypotheses\n(Isn‚Äôt that an eye-popping graphic?)\nThe toolbox of contrast coding schemes is deep but also confusing. Whenever I step away from R‚Äôs default contrast coding, I usually have these pages open to help me: some tutorial on a UCLA page, Lisa DeBruine‚Äôs comparison article, and the menu of contrast schemes in emmeans. So, let‚Äôs review the basics by looking at R‚Äôs default contrast coding scheme."
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html#the-default-dummy-coding",
    "href": "posts/bayesian-ordering-constraint/index.html#the-default-dummy-coding",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "The default: dummy coding",
    "text": "The default: dummy coding\nBy default, R will code categorical variables in a regression model using ‚Äútreatment‚Äù or ‚Äúdummy‚Äù coding. In this scheme,\n\nThe intercept is the mean of one of the category levels (the reference level)\nParameters estimate the difference between each other level and the reference level\n\nLet‚Äôs fit a simple linear model and work through the parameter meanings:\n\nm1 &lt;- lm(C14 ~ 1 + Layer, table1)\ncoef(m1)\n#&gt; (Intercept)      LayerC      LayerE      LayerG      LayerI \n#&gt;  -5670.6667   -137.3333   -223.6667   -438.3333   -628.3333\n\nHere, the (Intercept) is the mean of the reference level, and the reference level is the level of the categorical variable not listed in the other parameter names (LayerB). Each of the other parameters is a difference from that reference level. Layer C‚Äôs mean is (Intercept) + LayerC. The model.matrix() shows how these categorical variables are coded in the model‚Äôs design/contrast matrix:\n\n# Matrix has 1 row per observation but we just want 1 per category level\nmat_m1 &lt;- m1 |&gt; \n  model.matrix() |&gt;\n  unique()\nmat_m1\n#&gt;    (Intercept) LayerC LayerE LayerG LayerI\n#&gt; 1            1      0      0      0      0\n#&gt; 4            1      1      0      0      0\n#&gt; 6            1      0      1      0      0\n#&gt; 9            1      0      0      1      0\n#&gt; 11           1      0      0      0      1\n\nThe (Intercept) is the model constant, so naturally, it‚Äôs switched on (equals 1) for every row. Each of the other columns are indicator variables. layerC turns on for the layer C rows, layerE turns on for layer E rows, and so on.\nMatrix multiplying the contrast matrix by the model coefficients will compute the mean values of each layer.\n\\[\n\\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol{\\beta}\n\\]\nThink of this equation as a contract for a contrast coding scheme: Multiplying the contrast matrix by the model coefficients should give us the means of the category levels.\n\nmat_m1 %*% coef(m1)\n#&gt;         [,1]\n#&gt; 1  -5670.667\n#&gt; 4  -5808.000\n#&gt; 6  -5894.333\n#&gt; 9  -6109.000\n#&gt; 11 -6299.000\n\n# Means by hand\naggregate(C14 ~ Layer, table1, mean)\n#&gt;   Layer       C14\n#&gt; 1     B -5670.667\n#&gt; 2     C -5808.000\n#&gt; 3     E -5894.333\n#&gt; 4     G -6109.000\n#&gt; 5     I -6299.000\n\nIf the matrix multiplication is too quick, here it is in slow motion where each row has been weighted (multiplied) by coefficients:\n\n# Sums of the rows are the means\nmat_m1 %*% diag(coef(m1))\n#&gt;         [,1]      [,2]      [,3]      [,4]      [,5]\n#&gt; 1  -5670.667    0.0000    0.0000    0.0000    0.0000\n#&gt; 4  -5670.667 -137.3333    0.0000    0.0000    0.0000\n#&gt; 6  -5670.667    0.0000 -223.6667    0.0000    0.0000\n#&gt; 9  -5670.667    0.0000    0.0000 -438.3333    0.0000\n#&gt; 11 -5670.667    0.0000    0.0000    0.0000 -628.3333\n\nSuccessive differences coding\nNow, let‚Äôs look at a different kind of coding: (reverse) successive differences coding. In this scheme:\n\nThe intercept is the mean of the levels means\nParameters estimate the difference between adjacent levels\nbut I have to reverse how the levels are ordered in the underlying factor() so that the differences are positive, comparing each layer with the one below it. (LayerB - LayerC should be positive).\n\nWe apply this coding by creating a new factor and setting the contrast(). R lets us set the contrast to the name of a function that computes contrasts, so we use \"contr.sdif\".\n\ncontr.sdif &lt;- MASS::contr.sdif\n\n# Reverse the factor levels\ntable1$LayerAlt &lt;- factor(table1$Layer, rev(levels(table1$Layer)))\n\ncontrasts(table1$LayerAlt) &lt;- \"contr.sdif\"\n\nThen we just fit the model as usual. As intended, the model‚Äôs coefficients are different.\n\nm2 &lt;- lm(C14 ~ 1 + LayerAlt, table1)\ncoef(m2)\n#&gt; (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C \n#&gt; -5956.20000   190.00000   214.66667    86.33333   137.33333\n\nWe can compute the mean of layer means and the layer differences by hand to confirm that the model parameters are computing what we expect.\n\n# Make a list so we can write out the diffs easily\nlayer_means &lt;- table1 |&gt; \n  split(~ Layer) |&gt; \n  lapply(function(x) mean(x$C14))\nstr(layer_means)\n#&gt; List of 5\n#&gt;  $ B: num -5671\n#&gt;  $ C: num -5808\n#&gt;  $ E: num -5894\n#&gt;  $ G: num -6109\n#&gt;  $ I: num -6299\n\ndata.frame(\n  model_coef = coef(m2),\n  by_hand = c(\n    mean(unlist(layer_means)),\n    layer_means$G - layer_means$I,\n    layer_means$E - layer_means$G,\n    layer_means$C - layer_means$E,\n    layer_means$B - layer_means$C\n  )\n)\n#&gt;              model_coef     by_hand\n#&gt; (Intercept) -5956.20000 -5956.20000\n#&gt; LayerAltG-I   190.00000   190.00000\n#&gt; LayerAltE-G   214.66667   214.66667\n#&gt; LayerAltC-E    86.33333    86.33333\n#&gt; LayerAltB-C   137.33333   137.33333\n\nBack to our contrast coding contract, we see that the contrast matrix matrix-multiplied by the model coefficients gives us the level means.\n\nmat_m2 &lt;- unique(model.matrix(m2))\n\nmat_m2 %*% coef(m2)\n#&gt;         [,1]\n#&gt; 1  -5670.667\n#&gt; 4  -5808.000\n#&gt; 6  -5894.333\n#&gt; 9  -6109.000\n#&gt; 11 -6299.000\n\n# By hand\naggregate(C14 ~ Layer, table1, mean)\n#&gt;   Layer       C14\n#&gt; 1     B -5670.667\n#&gt; 2     C -5808.000\n#&gt; 3     E -5894.333\n#&gt; 4     G -6109.000\n#&gt; 5     I -6299.000\n\nIt‚Äôs so clean and simple. We still get the level means and the parameters estimate specific comparisons of interest to us. So, how are the categorical variables and their differences coded in the model‚Äôs contrast matrix?\n\nmat_m2\n#&gt;    (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C\n#&gt; 1            1         0.2         0.4         0.6         0.8\n#&gt; 4            1         0.2         0.4         0.6        -0.2\n#&gt; 6            1         0.2         0.4        -0.4        -0.2\n#&gt; 9            1         0.2        -0.6        -0.4        -0.2\n#&gt; 11           1        -0.8        -0.6        -0.4        -0.2\n\nWait‚Ä¶ what? üòï"
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html#the-comparison-matrix",
    "href": "posts/bayesian-ordering-constraint/index.html#the-comparison-matrix",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "The Comparison Matrix",
    "text": "The Comparison Matrix\nWhen I first started drafting this post, I made it to this point and noped out for a few days. My curiosity did win out eventually, and I hit the books (remembered this tweet and this handout, watched this video, read this paper, and read section 9.1.2 in Applied Regression Analysis & Generalized Linear Models). Now, for the rest of the post.\nThe best formal, citable source for what I describe here is Schad and colleagues (2020), but what they call a ‚Äúhypothesis matrix‚Äù, I‚Äôm calling a comparison matrix. I do this for two reasons: 1) to get away from hypothesis testing mindset (see Figure 1) and 2) because we are using the hypothesis matrix to apply a constraint among parameter values (remember that?).\n{% include figure image_path=‚Äú2023-07-bayes-sign.jpeg‚Äù alt=‚ÄúIn this house, we beleive: Bayes is good, estimate with uncertainty is better than hypothesis testing, math is hard, sampling is easy, Bayesian estimation wtih informative priors is indistinguishable from data falsifications, and it kicks ass.‚Äù caption=‚ÄúFigure 1. The sign in my yard.‚Äù %}{: style=‚Äúmax-width: 66%; display: block; margin: 2em auto;‚Äù}\nIn this approach, we define the model parameters Œ≤ by matrix-multiplying the the comparison matrix C (which activates or weights different level means) and the levels means Œº.\n\\[\n\\mathbf{C}\\boldsymbol{\\mu} = \\boldsymbol{\\beta} \\\\\n\\begin{bmatrix}\n  \\textrm{weights for comparison 1} \\\\\n  \\textrm{weights for comparison 2} \\\\\n  \\textrm{weights for comparison 3} \\\\\n  \\cdots \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\mu_1 \\\\\n  \\mu_2 \\\\\n  \\mu_3 \\\\\n  \\cdots \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\beta_2 \\\\\n  \\cdots \\\\\n\\end{bmatrix}\n\\]\nSo, in the dummy-coded version of the model, we had the following comparison matrix:\n\\[\n\\mathbf{C}_\\text{dummy}\\boldsymbol{\\mu} = \\boldsymbol{\\beta}_\\text{dummy} \\\\\n\\begin{bmatrix}\n  1 & 0 & 0 & 0 & 0 \\\\\n  -1 & 1 & 0 & 0 & 0 \\\\\n  -1 & 0 & 1 & 0 & 0 \\\\\n  -1 & 0 & 0 & 1 & 0 \\\\\n  -1 & 0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\mu_{\\text{Layer B}} \\\\\n  \\mu_{\\text{Layer C}} \\\\\n  \\mu_{\\text{Layer E}} \\\\\n  \\mu_{\\text{Layer G}} \\\\\n  \\mu_{\\text{Layer I}} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\beta_0: \\mu_{\\text{Layer B}} \\\\\n  \\beta_1: \\mu_{\\text{Layer C}} - \\mu_{\\text{Layer B}} \\\\\n  \\beta_2: \\mu_{\\text{Layer E}} - \\mu_{\\text{Layer B}} \\\\\n  \\beta_3: \\mu_{\\text{Layer G}} - \\mu_{\\text{Layer B}} \\\\\n  \\beta_4: \\mu_{\\text{Layer I}} - \\mu_{\\text{Layer B}} \\\\\n\\end{bmatrix}\n\\]\nThe first row in C sets the Layer B as the reference value for the dummy coding. The second row turns on both Layer B and Layer C, but Layer B is negatively weighted. Thus, the corresponding model coefficient is the difference between Layers C and B.\nThe comparison matrix for the reverse successive difference contrast coding is similar. The first row activates all of the layers buts equally weights them, so we get a mean of means for the model intercept. Each row after the first is the difference between two layer means.\n\\[\n\\mathbf{C}_\\text{rev-diffs}\\boldsymbol{\\mu} = \\boldsymbol{\\beta}_\\text{rev-diffs} \\\\\n\\begin{bmatrix}\n  .2 & .2 & .2 & .2 & .2 \\\\\n  0 &  0 &  0 &  1 & -1 \\\\\n  0 &  0 &  1 & -1 &  0 \\\\\n  0 &  1 & -1 &  0 &  0 \\\\\n  1 & -1 &  0 &  0 &  0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\mu_{\\text{Layer B}} \\\\\n  \\mu_{\\text{Layer C}} \\\\\n  \\mu_{\\text{Layer E}} \\\\\n  \\mu_{\\text{Layer G}} \\\\\n  \\mu_{\\text{Layer I}} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\beta_0: \\text{mean of } \\mu \\\\\n  \\beta_1: \\mu_{\\text{Layer G}} - \\mu_{\\text{Layer I}} \\\\\n  \\beta_2: \\mu_{\\text{Layer E}} - \\mu_{\\text{Layer G}} \\\\\n  \\beta_3: \\mu_{\\text{Layer C}} - \\mu_{\\text{Layer E}} \\\\\n  \\beta_4: \\mu_{\\text{Layer B}} - \\mu_{\\text{Layer C}} \\\\\n\\end{bmatrix}\n\\]\nNow, here is the magic part üîÆ. Multiplying both sides by the inverse of the comparison matrix will set up a design matrix for the linear model which follows the contract for the contrast matrices I described above:\n\\[\n\\mathbf{C}\\boldsymbol{\\mu} = \\boldsymbol{\\beta} \\\\\n\\mathbf{C}^{-1}\\mathbf{C}\\boldsymbol{\\mu} = \\mathbf{C}^{-1}\\boldsymbol{\\beta} \\\\\n\\boldsymbol{\\mu} = \\mathbf{C}^{-1}\\boldsymbol{\\beta} \\\\\n\\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol{\\beta} \\\\\n\\]\nSo, we can invert1 our comparison matrix to get the model‚Äôs contrast matrix:\n\ncomparisons &lt;- c(\n  .2, .2, .2, .2, .2,\n   0,  0,  0,  1, -1,\n   0,  0,  1, -1,  0,\n   0,  1, -1,  0,  0,\n   1, -1,  0,  0,  0\n)\n\nmat_comparisons &lt;- matrix(comparisons, nrow = 5, byrow = TRUE)\nsolve(mat_comparisons)\n#&gt;      [,1] [,2] [,3] [,4] [,5]\n#&gt; [1,]    1  0.2  0.4  0.6  0.8\n#&gt; [2,]    1  0.2  0.4  0.6 -0.2\n#&gt; [3,]    1  0.2  0.4 -0.4 -0.2\n#&gt; [4,]    1  0.2 -0.6 -0.4 -0.2\n#&gt; [5,]    1 -0.8 -0.6 -0.4 -0.2\n\nmat_m2\n#&gt;    (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C\n#&gt; 1            1         0.2         0.4         0.6         0.8\n#&gt; 4            1         0.2         0.4         0.6        -0.2\n#&gt; 6            1         0.2         0.4        -0.4        -0.2\n#&gt; 9            1         0.2        -0.6        -0.4        -0.2\n#&gt; 11           1        -0.8        -0.6        -0.4        -0.2\n\nOr, perhaps more commonly, we can take the contrast matrix used by a model and recover the comparison matrix, which is a nice trick when we have R automatically set the contrast values for us:\n\n# Dummy coding example\nmat_m1\n#&gt;    (Intercept) LayerC LayerE LayerG LayerI\n#&gt; 1            1      0      0      0      0\n#&gt; 4            1      1      0      0      0\n#&gt; 6            1      0      1      0      0\n#&gt; 9            1      0      0      1      0\n#&gt; 11           1      0      0      0      1\nsolve(mat_m1)\n#&gt;              1 4 6 9 11\n#&gt; (Intercept)  1 0 0 0  0\n#&gt; LayerC      -1 1 0 0  0\n#&gt; LayerE      -1 0 1 0  0\n#&gt; LayerG      -1 0 0 1  0\n#&gt; LayerI      -1 0 0 0  1\n\n# Successive differences coding example\nmat_m2\n#&gt;    (Intercept) LayerAltG-I LayerAltE-G LayerAltC-E LayerAltB-C\n#&gt; 1            1         0.2         0.4         0.6         0.8\n#&gt; 4            1         0.2         0.4         0.6        -0.2\n#&gt; 6            1         0.2         0.4        -0.4        -0.2\n#&gt; 9            1         0.2        -0.6        -0.4        -0.2\n#&gt; 11           1        -0.8        -0.6        -0.4        -0.2\nsolve(mat_m2)\n#&gt;               1    4    6    9   11\n#&gt; (Intercept) 0.2  0.2  0.2  0.2  0.2\n#&gt; LayerAltG-I 0.0  0.0  0.0  1.0 -1.0\n#&gt; LayerAltE-G 0.0  0.0  1.0 -1.0  0.0\n#&gt; LayerAltC-E 0.0  1.0 -1.0  0.0  0.0\n#&gt; LayerAltB-C 1.0 -1.0  0.0  0.0  0.0\n\nAs I said earlier, there are all kinds of contrast coding schemes which allow us to define the model parameters in terms of specific comparisons, and this post only mentions two such schemes (dummy coding and a reversed version of successive differences coding)."
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html#finally-in-layer-i-of-this-post-the-brms-model",
    "href": "posts/bayesian-ordering-constraint/index.html#finally-in-layer-i-of-this-post-the-brms-model",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Finally, in Layer I of this post, the brms model",
    "text": "Finally, in Layer I of this post, the brms model\nNow that we know about contrasts, and how they let us define model parameters in terms of the comparisons we want to make, we can use this technique to enforce an ordering constraint.\nWe set up our model as in Ben-Shachar‚Äôs post, but here we set a prior for normal(500, 250) on the non-intercept coefficients with a lower-bound of 0 lb = 0 to enforce the ordering constraint.\n\nlibrary(brms)\npriors &lt;- \n  set_prior(\"normal(-5975, 1000)\", class = \"Intercept\") + \n  set_prior(\"normal(500, 250)\", class = \"b\", lb = 0) +\n  set_prior(\"exponential(0.01)\", class = \"sigma\")\n\nvalidate_prior(\n  priors,\n  bf(C14 | se(error, sigma = TRUE) ~ 1 + LayerAlt),\n  data = table1\n)\n#&gt;                prior     class        coef group resp dpar nlpar lb ub\n#&gt;     normal(500, 250)         b                                    0   \n#&gt;     normal(500, 250)         b LayerAltBMC                        0   \n#&gt;     normal(500, 250)         b LayerAltCME                        0   \n#&gt;     normal(500, 250)         b LayerAltEMG                        0   \n#&gt;     normal(500, 250)         b LayerAltGMI                        0   \n#&gt;  normal(-5975, 1000) Intercept                                        \n#&gt;    exponential(0.01)     sigma                                    0   \n#&gt;        source\n#&gt;          user\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;  (vectorized)\n#&gt;          user\n#&gt;          user\n\nWe fit the model:\n\nm3 &lt;- brm(\n  bf(C14 | se(error, sigma = TRUE) ~ 1 + LayerAlt),\n  family = gaussian(\"identity\"),\n  prior = priors,\n  data = table1,\n  seed = 4321,\n  backend = \"cmdstanr\",\n  cores = 4, \n  # caching\n  file = \"2023-07-03\", \n  file_refit = \"on_change\"\n)\n\nWe can see that the level differences are indeed positive with 95% intervals of positive values.\n\nsummary(m3)\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: C14 | se(error, sigma = TRUE) ~ 1 + LayerAlt \n#&gt;    Data: table1 (Number of observations: 12) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Population-Level Effects: \n#&gt;             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept   -5957.60     27.91 -6011.89 -5900.71 1.00     1964     1715\n#&gt; LayerAltGMI   211.00     82.29    51.67   378.86 1.00     1693      939\n#&gt; LayerAltEMG   206.15     71.30    68.47   349.07 1.00     1937     1185\n#&gt; LayerAltCME   105.55     62.84     7.90   243.81 1.00     1377     1023\n#&gt; LayerAltBMC   145.95     65.13    23.63   279.12 1.00     1684      857\n#&gt; \n#&gt; Family Specific Parameters: \n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma    79.03     26.95    41.05   142.49 1.00     1651     2149\n#&gt; \n#&gt; Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).\nbayesplot::mcmc_intervals(m3, regex_pars = \"Layer\")\n\n\n\nEstimates of the level differences.\n\n\n\n\nconditional_effects(m3)\n\n\n\nConditional means for each layer."
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html#normally-i-dont-think-you-need-contrast-codes",
    "href": "posts/bayesian-ordering-constraint/index.html#normally-i-dont-think-you-need-contrast-codes",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Normally, I don‚Äôt think you need contrast codes",
    "text": "Normally, I don‚Äôt think you need contrast codes\nMy general advice for contrast coding is to just fit the model and then have the software compute the appropriate estimates and comparisons afterwards on the outcome scale. For example, emmeans can take a fitted model, run requested comparisons, and handle multiple comparisons and p-value adjustments for us. marginaleffects probably does this too. (I really need to play with it.) And in a Bayesian model, we can compute comparisons of interest by doing math on the posterior samples (estimating things and computing differences and summarizing the distribution of the differences), but this particular model, where the coding was needed to impose the prior ordering constraint, ruled out the posterior post-processing approach.\n\nSession info:\n\n.session_info\n#&gt; ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt;  setting         value\n#&gt;  version         R version 4.3.0 (2023-04-21 ucrt)\n#&gt;  os              Windows 11 x64 (build 22621)\n#&gt;  system          x86_64, mingw32\n#&gt;  ui              RTerm\n#&gt;  language        (EN)\n#&gt;  collate         English_United States.utf8\n#&gt;  ctype           English_United States.utf8\n#&gt;  tz              America/Chicago\n#&gt;  date            2023-07-19\n#&gt;  pandoc          3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt;  stan (rstan)    2.26.1\n#&gt;  stan (cmdstanr) 2.32.0\n#&gt; \n#&gt; ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt;  ! package        * version date (UTC) lib source\n#&gt;    abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.0)\n#&gt;    backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n#&gt;    base64enc        0.1-3   2015-07-28 [1] CRAN (R 4.3.0)\n#&gt;    bayesplot        1.10.0  2022-11-16 [1] CRAN (R 4.3.0)\n#&gt;    bridgesampling   1.1-2   2021-04-16 [1] CRAN (R 4.3.0)\n#&gt;    brms           * 2.19.0  2023-03-14 [1] CRAN (R 4.3.0)\n#&gt;    Brobdingnag      1.2-9   2022-10-19 [1] CRAN (R 4.3.0)\n#&gt;    callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.0)\n#&gt;    checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.0)\n#&gt;    cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;    cmdstanr         0.5.3   2023-04-24 [1] github (stan-dev/cmdstanr)\n#&gt;    coda             0.19-4  2020-09-30 [1] CRAN (R 4.3.0)\n#&gt;    codetools        0.2-19  2023-02-01 [2] CRAN (R 4.3.0)\n#&gt;    colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;    colourpicker     1.2.0   2022-10-28 [1] CRAN (R 4.3.0)\n#&gt;    crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.0)\n#&gt;    crosstalk        1.2.0   2021-11-04 [1] CRAN (R 4.3.0)\n#&gt;    curl             5.0.1   2023-06-07 [1] CRAN (R 4.3.0)\n#&gt;    digest           0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;    distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;    dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;    DT               0.28    2023-05-18 [1] CRAN (R 4.3.0)\n#&gt;    dygraphs         1.1.1.6 2018-07-11 [1] CRAN (R 4.3.0)\n#&gt;    ellipsis         0.3.2   2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;    emmeans          1.8.7   2023-06-23 [1] CRAN (R 4.3.1)\n#&gt;    estimability     1.4.1   2022-08-05 [1] CRAN (R 4.3.0)\n#&gt;    evaluate         0.21    2023-05-05 [1] CRAN (R 4.3.0)\n#&gt;    fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.0)\n#&gt;    farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n#&gt;    fastmap          1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;    forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n#&gt;    generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;    ggplot2        * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n#&gt;    glue             1.6.2   2022-02-24 [1] CRAN (R 4.3.0)\n#&gt;    gridExtra        2.3     2017-09-09 [1] CRAN (R 4.3.0)\n#&gt;    gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;    gtools           3.9.4   2022-11-27 [1] CRAN (R 4.3.0)\n#&gt;    hms              1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;    htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;    htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.0)\n#&gt;    httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.0)\n#&gt;    igraph           1.5.0   2023-06-16 [1] CRAN (R 4.3.1)\n#&gt;    inline           0.3.19  2021-05-31 [1] CRAN (R 4.3.0)\n#&gt;    jsonlite         1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n#&gt;    knitr            1.43    2023-05-25 [1] CRAN (R 4.3.0)\n#&gt;    labeling         0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n#&gt;    later            1.3.1   2023-05-02 [1] CRAN (R 4.3.0)\n#&gt;    lattice          0.21-8  2023-04-05 [2] CRAN (R 4.3.0)\n#&gt;    lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n#&gt;    loo              2.6.0   2023-03-31 [1] CRAN (R 4.3.0)\n#&gt;    lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;    magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;    markdown         1.7     2023-05-16 [1] CRAN (R 4.3.0)\n#&gt;    MASS           * 7.3-60  2023-05-04 [1] CRAN (R 4.3.0)\n#&gt;    Matrix           1.6-0   2023-07-08 [1] CRAN (R 4.3.1)\n#&gt;    matrixStats      1.0.0   2023-06-02 [1] CRAN (R 4.3.0)\n#&gt;    mime             0.12    2021-09-28 [1] CRAN (R 4.3.0)\n#&gt;    miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.0)\n#&gt;    munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n#&gt;    mvtnorm          1.2-2   2023-06-08 [1] CRAN (R 4.3.1)\n#&gt;    nlme             3.1-162 2023-01-31 [2] CRAN (R 4.3.0)\n#&gt;    pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;    pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n#&gt;    pkgconfig        2.0.3   2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;    plyr             1.8.8   2022-11-11 [1] CRAN (R 4.3.0)\n#&gt;    posterior        1.4.1   2023-03-14 [1] CRAN (R 4.3.0)\n#&gt;    prettyunits      1.1.1   2020-01-24 [1] CRAN (R 4.3.0)\n#&gt;    processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n#&gt;    promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.0)\n#&gt;    ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.0)\n#&gt;    purrr          * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n#&gt;    R6               2.5.1   2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;    ragg             1.2.5   2023-01-12 [1] CRAN (R 4.3.0)\n#&gt;    Rcpp           * 1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;  D RcppParallel     5.1.7   2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;    readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;    reshape2         1.4.4   2020-04-09 [1] CRAN (R 4.3.0)\n#&gt;    rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n#&gt;    rmarkdown        2.23    2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;    rstan            2.26.22 2023-05-02 [1] local\n#&gt;    rstantools       2.3.1   2023-03-30 [1] CRAN (R 4.3.0)\n#&gt;    rstudioapi       0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;    scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n#&gt;    sessioninfo      1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n#&gt;    shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;    shinyjs          2.1.0   2021-12-23 [1] CRAN (R 4.3.0)\n#&gt;    shinystan        2.6.0   2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;    shinythemes      1.2.0   2021-01-25 [1] CRAN (R 4.3.0)\n#&gt;    StanHeaders      2.26.27 2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;    stringi          1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;    stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n#&gt;    systemfonts      1.0.4   2022-02-11 [1] CRAN (R 4.3.0)\n#&gt;    tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.0)\n#&gt;    textshaping      0.3.6   2021-10-13 [1] CRAN (R 4.3.0)\n#&gt;    threejs          0.3.3   2020-01-21 [1] CRAN (R 4.3.0)\n#&gt;    tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;    tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n#&gt;    tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;    tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n#&gt;    timechange       0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;    tzdb             0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n#&gt;    utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.0)\n#&gt;    V8               4.3.2   2023-06-30 [1] CRAN (R 4.3.1)\n#&gt;    vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;    withr            2.5.0   2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;    xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;    xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.0)\n#&gt;    xts              0.13.1  2023-04-16 [1] CRAN (R 4.3.0)\n#&gt;    yaml             2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;    zoo              1.8-12  2023-04-13 [1] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n#&gt;  [2] C:/Program Files/R/R-4.3.0/library\n#&gt; \n#&gt;  D ‚îÄ‚îÄ DLL MD5 mismatch, broken installation.\n#&gt; \n#&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  },
  {
    "objectID": "posts/bayesian-ordering-constraint/index.html#footnotes",
    "href": "posts/bayesian-ordering-constraint/index.html#footnotes",
    "title": "Ordering constraints in brms using contrast coding",
    "section": "Footnotes",
    "text": "Footnotes\n\nI use solve() here for the inversion, but Schad and colleagues (2020) use the generalized inverse MASS::ginv() or matlib::Ginv(). solve() only works on square matrices, but the generalized inverse works on non-square matrices.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/confusion-matrix-late-talkers/index.html",
    "href": "posts/confusion-matrix-late-talkers/index.html",
    "title": "Confusion matrix statistics on late talker diagnoses",
    "section": "",
    "text": "How many late talkers are just late bloomers? More precisely, how many children identified as late talkers at 18 months catch up to the normal range by one year later? This is an important question. From a clinical perspective, we want to support children with language delays, but it is also inefficient to spend resources fixing a self-correcting problem.\nFernald and Marchman (2012) touch on this question. Children falling below the 20th percentile in vocabulary score at 18 months were labeled ‚Äúlate talkers‚Äù. These children, along with a control group of timely-talkers, participated in an eyetracking study at 18 months and had their vocabulary measured every 3 months until 30 months of age.\nIn their sample, 22 of 36 late talkers were late bloomers, catching up to the normal vocabulary range at 30 months, and 42 of 46 timely talkers remained in the normal range of vocab development. The authors later report that eyetracking reaction times at 18 months predicted rates of vocabulary growth in both groups. In particular, the late-bloomers were significantly faster than the children who did not catch up.\nThe authors repeatedly report confusion matrix statistics on different subsets of the data. Which make sense: The question of late bloomers is also a question about the positive predictive value of a late-talker diagnosis. In the majority of cases, a ‚Äúlate talker‚Äù label at 18 months did not predict continued delay one year later. Therefore, the diagnosis has poor positive predictive value (14/36 = 39%)."
  },
  {
    "objectID": "posts/confusion-matrix-late-talkers/index.html#confusion-matrix-measures-in-r",
    "href": "posts/confusion-matrix-late-talkers/index.html#confusion-matrix-measures-in-r",
    "title": "Confusion matrix statistics on late talker diagnoses",
    "section": "Confusion Matrix Measures in R",
    "text": "Confusion Matrix Measures in R\nI would like to report similar classification quantities in my own analyses, so I figured out how to reproduce their results in R. And it‚Äôs as simple as calling the caret::confusionMatrix() function in the caret package.\nFirst, let‚Äôs re-create their data. We‚Äôll make a long dataframe with one row per child reported in the study. We will have fields for each child‚Äôs initial Group (late talking or within-normal-limits at 18 months), their Predicted group (assuming late-talking children remain delayed), and the observed Outcome.\n\nlibrary(dplyr)\n\n# LT: late talking\n# WNL: within normal limits\ngroups &lt;- c(\"WNL at 18m\", \"LT at 18m\")\noutcomes &lt;- c(\"WNL at 30m\", \"Delayed at 30m\")\n\n# Counts from paper\nlt_still_delayed &lt;- 14\nlt_bloomed &lt;- 22\n\nwnl_still_wnl &lt;- 42\nwnl_delayed &lt;- 4\n\n# Reproduce their data-set (one row per reported child)\nwnl_data &lt;- tibble(\n  Group = groups[1],\n  Predicted = outcomes[1],\n  Outcome = rep(outcomes, times = c(wnl_still_wnl, wnl_delayed))\n)\n\nlt_data &lt;- tibble(\n  Group = \"LT at 18m\",\n  Outcome = rep(outcomes, times = c(lt_bloomed, lt_still_delayed)),\n  Predicted = outcomes[2]\n)\n\nall_kids &lt;- bind_rows(wnl_data, lt_data) %&gt;%\n  mutate(ChildID = seq_along(Outcome)) %&gt;% \n  select(ChildID, Group, Predicted, Outcome) %&gt;% \n  mutate(\n    Predicted = factor(Predicted, outcomes),\n    Outcome = factor(Outcome, outcomes)\n  )\n\nWhat we have looks like a real data-set now.\n\n\n\n\nall_kids %&gt;% \n  sample_n(8, replace = FALSE) %&gt;% \n  arrange(Group, Predicted, Outcome)\n#&gt; # A tibble: 8 √ó 4\n#&gt;   ChildID Group      Predicted      Outcome   \n#&gt;     &lt;int&gt; &lt;chr&gt;      &lt;fct&gt;          &lt;fct&gt;     \n#&gt; 1      47 LT at 18m  Delayed at 30m WNL at 30m\n#&gt; 2      52 LT at 18m  Delayed at 30m WNL at 30m\n#&gt; 3      60 LT at 18m  Delayed at 30m WNL at 30m\n#&gt; 4       1 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 5      16 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 6      19 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 7      34 WNL at 18m WNL at 30m     WNL at 30m\n#&gt; 8      27 WNL at 18m WNL at 30m     WNL at 30m\n\nNext, we just call caret::confusionMatrix() on the predicted values and the reference values.\n\nconf_mat &lt;- caret::confusionMatrix(all_kids$Predicted, all_kids$Outcome)\nconf_mat\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;                 Reference\n#&gt; Prediction       WNL at 30m Delayed at 30m\n#&gt;   WNL at 30m             42              4\n#&gt;   Delayed at 30m         22             14\n#&gt;                                           \n#&gt;                Accuracy : 0.6829          \n#&gt;                  95% CI : (0.5708, 0.7813)\n#&gt;     No Information Rate : 0.7805          \n#&gt;     P-Value [Acc &gt; NIR] : 0.9855735       \n#&gt;                                           \n#&gt;                   Kappa : 0.3193          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 0.0008561       \n#&gt;                                           \n#&gt;             Sensitivity : 0.6562          \n#&gt;             Specificity : 0.7778          \n#&gt;          Pos Pred Value : 0.9130          \n#&gt;          Neg Pred Value : 0.3889          \n#&gt;              Prevalence : 0.7805          \n#&gt;          Detection Rate : 0.5122          \n#&gt;    Detection Prevalence : 0.5610          \n#&gt;       Balanced Accuracy : 0.7170          \n#&gt;                                           \n#&gt;        'Positive' Class : WNL at 30m      \n#&gt; \n\n\n\n\nHere, we can confirm the positive predictive value (true positives / positive calls)1 is 14/36 = 0.913. The negative predictive value is noteworthy; most children not diagnosed as late talkers did not show a delay one year later (NPV = 42/46 = 0.3889).\n\nSession info:\n\n.session_info\n#&gt; ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt;  setting  value\n#&gt;  version  R version 4.3.0 (2023-04-21 ucrt)\n#&gt;  os       Windows 11 x64 (build 22621)\n#&gt;  system   x86_64, mingw32\n#&gt;  ui       RTerm\n#&gt;  language (EN)\n#&gt;  collate  English_United States.utf8\n#&gt;  ctype    English_United States.utf8\n#&gt;  tz       America/Chicago\n#&gt;  date     2023-07-14\n#&gt;  pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt; \n#&gt; ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt;  package      * version    date (UTC) lib source\n#&gt;  caret          6.0-94     2023-03-21 [1] CRAN (R 4.3.1)\n#&gt;  class          7.3-21     2023-01-23 [2] CRAN (R 4.3.0)\n#&gt;  cli            3.6.1      2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;  codetools      0.2-19     2023-02-01 [2] CRAN (R 4.3.0)\n#&gt;  colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;  data.table     1.14.8     2023-02-17 [1] CRAN (R 4.3.0)\n#&gt;  digest         0.6.33     2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;  dplyr        * 1.1.2      2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;  e1071          1.7-13     2023-02-01 [1] CRAN (R 4.3.1)\n#&gt;  evaluate       0.21       2023-05-05 [1] CRAN (R 4.3.0)\n#&gt;  fansi          1.0.4      2023-01-22 [1] CRAN (R 4.3.0)\n#&gt;  fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;  foreach        1.5.2      2022-02-02 [1] CRAN (R 4.3.1)\n#&gt;  future         1.33.0     2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;  future.apply   1.11.0     2023-05-21 [1] CRAN (R 4.3.1)\n#&gt;  generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;  ggplot2        3.4.2      2023-04-03 [1] CRAN (R 4.3.0)\n#&gt;  globals        0.16.2     2022-11-21 [1] CRAN (R 4.3.0)\n#&gt;  glue           1.6.2      2022-02-24 [1] CRAN (R 4.3.0)\n#&gt;  gower          1.0.1      2022-12-22 [1] CRAN (R 4.3.0)\n#&gt;  gtable         0.3.3      2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;  hardhat        1.3.0      2023-03-30 [1] CRAN (R 4.3.1)\n#&gt;  htmltools      0.5.5      2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;  htmlwidgets    1.6.2      2023-03-17 [1] CRAN (R 4.3.0)\n#&gt;  ipred          0.9-14     2023-03-09 [1] CRAN (R 4.3.1)\n#&gt;  iterators      1.0.14     2022-02-05 [1] CRAN (R 4.3.1)\n#&gt;  jsonlite       1.8.7      2023-06-29 [1] CRAN (R 4.3.1)\n#&gt;  knitr          1.43       2023-05-25 [1] CRAN (R 4.3.0)\n#&gt;  lattice        0.21-8     2023-04-05 [2] CRAN (R 4.3.0)\n#&gt;  lava           1.7.2.1    2023-02-27 [1] CRAN (R 4.3.1)\n#&gt;  lifecycle      1.0.3      2022-10-07 [1] CRAN (R 4.3.0)\n#&gt;  listenv        0.9.0      2022-12-16 [1] CRAN (R 4.3.0)\n#&gt;  lubridate      1.9.2      2023-02-10 [1] CRAN (R 4.3.0)\n#&gt;  magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;  MASS           7.3-60     2023-05-04 [1] CRAN (R 4.3.0)\n#&gt;  Matrix         1.6-0      2023-07-08 [1] CRAN (R 4.3.1)\n#&gt;  ModelMetrics   1.2.2.2    2020-03-17 [1] CRAN (R 4.3.1)\n#&gt;  munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n#&gt;  nlme           3.1-162    2023-01-31 [2] CRAN (R 4.3.0)\n#&gt;  nnet           7.3-19     2023-05-03 [1] CRAN (R 4.3.0)\n#&gt;  parallelly     1.36.0     2023-05-26 [1] CRAN (R 4.3.0)\n#&gt;  pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;  pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;  plyr           1.8.8      2022-11-11 [1] CRAN (R 4.3.0)\n#&gt;  pROC           1.18.4     2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;  prodlim        2023.03.31 2023-04-02 [1] CRAN (R 4.3.1)\n#&gt;  proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.1)\n#&gt;  purrr          1.0.1      2023-01-10 [1] CRAN (R 4.3.0)\n#&gt;  R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;  ragg           1.2.5      2023-01-12 [1] CRAN (R 4.3.0)\n#&gt;  Rcpp           1.0.11     2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;  recipes        1.0.6      2023-04-25 [1] CRAN (R 4.3.1)\n#&gt;  reshape2       1.4.4      2020-04-09 [1] CRAN (R 4.3.0)\n#&gt;  rlang          1.1.1      2023-04-28 [1] CRAN (R 4.3.0)\n#&gt;  rmarkdown      2.23       2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;  rpart          4.1.19     2022-10-21 [2] CRAN (R 4.3.0)\n#&gt;  rstudioapi     0.15.0     2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;  scales         1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n#&gt;  sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n#&gt;  stringi        1.7.12     2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;  stringr        1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n#&gt;  survival       3.5-5      2023-03-12 [2] CRAN (R 4.3.0)\n#&gt;  systemfonts    1.0.4      2022-02-11 [1] CRAN (R 4.3.0)\n#&gt;  textshaping    0.3.6      2021-10-13 [1] CRAN (R 4.3.0)\n#&gt;  tibble         3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;  tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;  timechange     0.2.0      2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;  timeDate       4022.108   2023-01-07 [1] CRAN (R 4.3.0)\n#&gt;  utf8           1.2.3      2023-01-31 [1] CRAN (R 4.3.0)\n#&gt;  vctrs          0.6.3      2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;  withr          2.5.0      2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;  xfun           0.39       2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;  yaml           2.3.7      2023-01-23 [1] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n#&gt;  [2] C:/Program Files/R/R-4.3.0/library\n#&gt; \n#&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  },
  {
    "objectID": "posts/confusion-matrix-late-talkers/index.html#footnotes",
    "href": "posts/confusion-matrix-late-talkers/index.html#footnotes",
    "title": "Confusion matrix statistics on late talker diagnoses",
    "section": "Footnotes",
    "text": "Footnotes\n\nTechnically, caret uses the sensitivity, specificity and prevalence form of the PPV calculation.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/migrating-from-jekyll-to-quarto/index.html",
    "href": "posts/migrating-from-jekyll-to-quarto/index.html",
    "title": "playing with quatro",
    "section": "",
    "text": "This is the not first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts.\nOkay!\n\nI want a banner image on top like the old site with a credit to the source.\nI had to put fig-width: 8 under format &gt; html in _quarto.yml. I also pushed other knitr options here:\n    fig-asp: 0.618\n    fig-dpi: 300\n    fig-align: \"center\"\n    fig-cap-location: \"margin\"\nI got to opt into fig-cap-location: \"margin\".\nI had to remove the old knitr _cached stuff. Still need a different solution.\nI don‚Äôt want the green links. If I set the link color, that still leaves the green link color in the table of contents.\nAll of my old manual downlit attempts are styled differently than actual downlit urls.\nI want a slightly larger font size.\nI want underlined downlit links.\nI have to convert fig.alt and fig.cap to here.\n#| fig-alt: \"Examples of `plot_dist()`\"\n#| fig-cap:"
  },
  {
    "objectID": "posts/recent-adventures-with-lazyeval/index.html",
    "href": "posts/recent-adventures-with-lazyeval/index.html",
    "title": "Recent adventures with lazyeval",
    "section": "",
    "text": "The lazyeval package is a toolset for performing nonstandard evaluation in R. Nonstandard evaluation refers to any situation where something special happens with how user input or code is evaluated.\nFor example, the library() function doesn‚Äôt evaluate variables. In the example below, I try to trick library() into loading a fake package called evil_package by assigning to the package name lazyeval. In other words, we have the expression lazyeval and its value is \"evil_package\".\nprint(.packages())\n#&gt; [1] \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\"  \"methods\"  \n#&gt; [7] \"base\"\n\nlazyeval &lt;- \"evil_package\"\nlibrary(lazyeval)\n\n# The lazyeval package is loaded now.\nprint(.packages())\n#&gt; [1] \"lazyeval\"  \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n#&gt; [7] \"methods\"   \"base\"\nBut this gambit doesn‚Äôt work because library() did something special: It didn‚Äôt evaluate the expression lazyeval. In the source code of library(), there is a line package &lt;- as.character(substitute(package)) which replaces the value of package with a character version of the expression that the user wrote.\nThat‚Äôs a simple example of nonstandard evaluation, but it‚Äôs pervasive. It‚Äôs why you never have to quote column names in dplyr or ggplot2. In this post, I present some recent examples where I decided to use the lazyeval package to do something nonstandard. These examples are straight out of the lazyeval vignette in terms of complexity, but that‚Äôs fine. We all have to start somewhere."
  },
  {
    "objectID": "posts/recent-adventures-with-lazyeval/index.html#capturing-expressions",
    "href": "posts/recent-adventures-with-lazyeval/index.html#capturing-expressions",
    "title": "Recent adventures with lazyeval",
    "section": "Capturing expressions",
    "text": "Capturing expressions\nPlot titles. While reading the book Machine Learning For Hackers, I wanted to plot random numbers generated by probability distributions discussed by the book. I used the lazyeval::expr_text() function to capture the command used to generate the numbers and write it as the title of the plot.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(ggplot2)\n\nplot_dist &lt;- function(xs) {\n  data &lt;- tibble(x = xs)\n  ggplot(data) +\n    aes(x = x) +\n    geom_density() +\n    ggtitle(lazyeval::expr_text(xs)) \n}\n\nplot_dist(rcauchy(n = 250, location = 0, scale = 1))\n\n\n\nExamples of plot_dist()\n\n\nplot_dist(rgamma(n = 25000, shape = 5, rate = .5))\n\n\n\nExamples of plot_dist()\n\n\nplot_dist(rexp(n = 25000, rate = .5))\n\n\n\nExamples of plot_dist()\n\n\n\nLess fussy warning messages. I recently inherited some code where there were custom warning messages based on the input. The code threw a warning whenever a duplicate participant ID was found in a survey. It went something like this:\n\n# some dummy data\nstudy1 &lt;- tibble(\n  id = c(1, 2, 3, 4), \n  response = c(\"b\", \"c\", \"a\", \"b\")\n)\n\nstudy2 &lt;- tibble(\n  id = c(1, 2, 2, 3, 1), \n  response = c(\"a\", \"a\", \"a\", \"b\", \"c\")\n)\n\nif (anyDuplicated(study1$id)) {\n  warning(\"Duplicate IDs found in Study1\", call. = FALSE)\n}\n\nif (anyDuplicated(study2$id)) {\n  warning(\"Duplicate IDs found in Study2\", call. = FALSE)\n}\n#&gt; Warning: Duplicate IDs found in Study2\n\nTo extend this code to a new study, one would just copy-and-paste and update the if statement‚Äôs condition and warning messages. Like so:\n\nstudy3 &lt;- tibble(\n  id = c(1, 2, 3, 2), \n  response = c(\"b\", \"c\", \"a\", \"b\")\n)\n\nif (anyDuplicated(study3$id)) {\n  warning(\"Duplicate IDs found in Study2\", call. = FALSE)\n}\n#&gt; Warning: Duplicate IDs found in Study2\n\nWait, that‚Äôs not right! I forgot to update the warning message‚Ä¶\nThis setup is too brittle for me, so I abstracted the procedure into a function. First, I wrote a helper function to print out duplicates elements in a vector. This helper will let us make the warning message a little more informative.\n\n# Print out duplicated elements in a vector\nprint_duplicates &lt;- function(xs) {\n  duplicated &lt;- xs[duplicated(xs)]\n  duplicated %&gt;% sort() %&gt;% unique() %&gt;% paste0(collapse = \", \")\n}\n\nprint_duplicates(study2$id)\n#&gt; [1] \"1, 2\"\n\nNext, I wrote a function to issue the warnings. I used lazyeval::expr_label() convert the user-inputted expression into a string wrapped in backticks.\n\n# Print a warning if duplicates are found in a vector\nwarn_duplicates &lt;- function(xs) {\n  if (anyDuplicated(xs)) {\n    # Get what the user wrote for the xs argument\n    actual_xs &lt;- lazyeval::expr_label(xs)\n    msg &lt;- paste0(\n      \"Duplicate entries in \", actual_xs, \": \", print_duplicates(xs)\n    )\n    warning(msg, call. = FALSE)\n  }\n  invisible(NULL)\n}\n\nwarn_duplicates(study1$id)\nwarn_duplicates(study2$id)\n#&gt; Warning: Duplicate entries in `study2$id`: 1, 2\nwarn_duplicates(study3$id)\n#&gt; Warning: Duplicate entries in `study3$id`: 2\n\nThe advantage of this approach is that the warning is a generic message that can work on any input. But in a funny way, the warning is also customized for the input because it includes the input printed verbatim.\nAn aside: In plotting, I used lazyeval::expr_text(), but here I used lazyeval::expr_label(). The two differ slightly. Namely, expr_label() surrounds the captured expression with backticks to indicate that expression is code:\n\nlazyeval::expr_text(stop())\n#&gt; [1] \"stop()\"\nlazyeval::expr_label(stop())\n#&gt; [1] \"`stop()`\"\n\n# 2021-01-05: rlang requires you to separate the quoting from the quoting\nrlang::quo_text(quote(stop()))\n#&gt; [1] \"stop()\"\nrlang::quo_label(quote(stop()))\n#&gt; [1] \"`stop()`\"\n\nrlang::quo_text(rlang::quo(stop()))\n#&gt; [1] \"stop()\"\nrlang::quo_label(rlang::quo(stop()))\n#&gt; [1] \"`stop()`\""
  },
  {
    "objectID": "posts/recent-adventures-with-lazyeval/index.html#asking-questions-about-a-posterior-distribution",
    "href": "posts/recent-adventures-with-lazyeval/index.html#asking-questions-about-a-posterior-distribution",
    "title": "Recent adventures with lazyeval",
    "section": "Asking questions about a posterior distribution",
    "text": "Asking questions about a posterior distribution\nI fit regression models with RStanARM. It lets me fit Bayesian models in Stan by writing conventional R modeling code. (Btw, I‚Äôm giving a tutorial on RStanARM in a month.)\nHere‚Äôs a model about some famous flowers.\n\nlibrary(rstanarm)\n#&gt; Loading required package: Rcpp\n#&gt; Warning: package 'Rcpp' was built under R version 4.3.1\n#&gt; This is rstanarm version 2.21.4\n#&gt; - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n#&gt; - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n#&gt; - For execution on a local, multicore CPU with excess RAM we recommend calling\n#&gt;   options(mc.cores = parallel::detectCores())\n\nmodel &lt;- stan_glm(\n  Petal.Width ~ Petal.Length * Species,\n  data = iris,\n  family = gaussian(), \n  prior = normal(0, 1),\n  seed = \"20230718\"\n)\n\nHere‚Äôs the essential relationship being explored.\n\nggplot(iris) + \n  aes(x = Petal.Length, y = Petal.Width, color = Species) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nThe model gives me 4000 samples from the posterior distribution of the model.\n\nsummary(model)\n#&gt; \n#&gt; Model Info:\n#&gt;  function:     stan_glm\n#&gt;  family:       gaussian [identity]\n#&gt;  formula:      Petal.Width ~ Petal.Length * Species\n#&gt;  algorithm:    sampling\n#&gt;  sample:       4000 (posterior sample size)\n#&gt;  priors:       see help('prior_summary')\n#&gt;  observations: 150\n#&gt;  predictors:   6\n#&gt; \n#&gt; Estimates:\n#&gt;                                  mean   sd   10%   50%   90%\n#&gt; (Intercept)                     0.0    0.2 -0.3   0.0   0.3 \n#&gt; Petal.Length                    0.2    0.1  0.0   0.2   0.3 \n#&gt; Speciesversicolor              -0.1    0.3 -0.5  -0.1   0.3 \n#&gt; Speciesvirginica                1.1    0.3  0.7   1.1   1.5 \n#&gt; Petal.Length:Speciesversicolor  0.2    0.1  0.0   0.2   0.4 \n#&gt; Petal.Length:Speciesvirginica   0.0    0.1 -0.2   0.0   0.2 \n#&gt; sigma                           0.2    0.0  0.2   0.2   0.2 \n#&gt; \n#&gt; Fit Diagnostics:\n#&gt;            mean   sd   10%   50%   90%\n#&gt; mean_PPD 1.2    0.0  1.2   1.2   1.2  \n#&gt; \n#&gt; The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n#&gt; \n#&gt; MCMC diagnostics\n#&gt;                                mcse Rhat n_eff\n#&gt; (Intercept)                    0.0  1.0   858 \n#&gt; Petal.Length                   0.0  1.0   862 \n#&gt; Speciesversicolor              0.0  1.0  1147 \n#&gt; Speciesvirginica               0.0  1.0  1165 \n#&gt; Petal.Length:Speciesversicolor 0.0  1.0   839 \n#&gt; Petal.Length:Speciesvirginica  0.0  1.0   829 \n#&gt; sigma                          0.0  1.0  2756 \n#&gt; mean_PPD                       0.0  1.0  3166 \n#&gt; log-posterior                  0.0  1.0  1394 \n#&gt; \n#&gt; For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\nAt the 2.5% quantile, the Petal.Length effect looks like zero or less than zero. What proportion of the Petal.Length effects (for setosa flowers) is positive?\nTo answer questions like this one in a convenient way, I wrote a function that takes a boolean expression about a model‚Äôs parameters and evaluates it inside of the data-frame summary of the model posterior distribution. lazyeval::f_eval() does the nonstandard evaluation: It evaluates an expression captured by a formula like ~ 0 &lt; Petal.Length inside of a list or data-frame. (Note that the mean of a logical vector is the proportion of the elements that are true.)\n\n# Get proportion of posterior samples satisfying some inequality\nposterior_proportion_ &lt;- function(model, inequality) {\n  draws &lt;- as.data.frame(model)\n  mean(lazyeval::f_eval(inequality, data = draws))\n}\n\nposterior_proportion_(model, ~ 0 &lt; Petal.Length)\n#&gt; [1] 0.87675\n\nBut all those tildes‚Ä¶ The final underscore in posterior_proportion_() follows a convention for distinguishing between nonstandard evaluation functions that require formulas and those that do not. In the dplyr package, for example, there is select_()/select()/, mutate_()/mutate(), and so on. We can do the formula-free form of this function by using lazyeval::f_capture() to capture the input expression as a formula. We then hand that off to the version of the function that takes a formula.\n\nposterior_proportion &lt;- function(model, expr) {\n  posterior_proportion_(model, lazyeval::f_capture(expr))\n}\n\nposterior_proportion(model, 0 &lt; Petal.Length)\n#&gt; [1] 0.87675\n\nHere‚Äôs another question: What proportion of the posterior of the Petal.Length effect for virginica flowers is positive? In classical models, we would change the reference level for the categorical variable, refit the model, and check the significance. But I don‚Äôt want to refit this model because that would repeat the MCMC sampling. (It takes about 30 seconds to fit this model after all!) I‚Äôll just ask the model for the sum of Petal.Length and Petal.Length:Speciesversicolor effects. That will give me the estimated Petal.Length effect but adjusted for the versicolor species.\n\nposterior_proportion(model, 0 &lt; Petal.Length + `Petal.Length:Speciesversicolor`)\n#&gt; [1] 1\n\nposterior_proportion(model, 0 &lt; Petal.Length + `Petal.Length:Speciesvirginica`)\n#&gt; [1] 1\n\n(The backticks around Petal.Length:Speciesversicolor here prevent the : symbol from being evaluated as an operator.)\n\nSession info:\n\n.session_info\n#&gt; ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt;  setting      value\n#&gt;  version      R version 4.3.0 (2023-04-21 ucrt)\n#&gt;  os           Windows 11 x64 (build 22621)\n#&gt;  system       x86_64, mingw32\n#&gt;  ui           RTerm\n#&gt;  language     (EN)\n#&gt;  collate      English_United States.utf8\n#&gt;  ctype        English_United States.utf8\n#&gt;  tz           America/Chicago\n#&gt;  date         2023-07-19\n#&gt;  pandoc       3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n#&gt;  stan (rstan) 2.26.1\n#&gt; \n#&gt; ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n#&gt;  ! package      * version  date (UTC) lib source\n#&gt;    base64enc      0.1-3    2015-07-28 [1] CRAN (R 4.3.0)\n#&gt;    bayesplot      1.10.0   2022-11-16 [1] CRAN (R 4.3.0)\n#&gt;    boot           1.3-28.1 2022-11-22 [2] CRAN (R 4.3.0)\n#&gt;    callr          3.7.3    2022-11-02 [1] CRAN (R 4.3.0)\n#&gt;    cli            3.6.1    2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;    codetools      0.2-19   2023-02-01 [2] CRAN (R 4.3.0)\n#&gt;    colorspace     2.1-0    2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;    colourpicker   1.2.0    2022-10-28 [1] CRAN (R 4.3.0)\n#&gt;    crayon         1.5.2    2022-09-29 [1] CRAN (R 4.3.0)\n#&gt;    crosstalk      1.2.0    2021-11-04 [1] CRAN (R 4.3.0)\n#&gt;    curl           5.0.1    2023-06-07 [1] CRAN (R 4.3.0)\n#&gt;    digest         0.6.33   2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;    dplyr        * 1.1.2    2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;    DT             0.28     2023-05-18 [1] CRAN (R 4.3.0)\n#&gt;    dygraphs       1.1.1.6  2018-07-11 [1] CRAN (R 4.3.0)\n#&gt;    ellipsis       0.3.2    2021-04-29 [1] CRAN (R 4.3.0)\n#&gt;    evaluate       0.21     2023-05-05 [1] CRAN (R 4.3.0)\n#&gt;    fansi          1.0.4    2023-01-22 [1] CRAN (R 4.3.0)\n#&gt;    farver         2.1.1    2022-07-06 [1] CRAN (R 4.3.0)\n#&gt;    fastmap        1.1.1    2023-02-24 [1] CRAN (R 4.3.0)\n#&gt;    generics       0.1.3    2022-07-05 [1] CRAN (R 4.3.0)\n#&gt;    ggplot2      * 3.4.2    2023-04-03 [1] CRAN (R 4.3.0)\n#&gt;    glue           1.6.2    2022-02-24 [1] CRAN (R 4.3.0)\n#&gt;    gridExtra      2.3      2017-09-09 [1] CRAN (R 4.3.0)\n#&gt;    gtable         0.3.3    2023-03-21 [1] CRAN (R 4.3.0)\n#&gt;    gtools         3.9.4    2022-11-27 [1] CRAN (R 4.3.0)\n#&gt;    htmltools      0.5.5    2023-03-23 [1] CRAN (R 4.3.0)\n#&gt;    htmlwidgets    1.6.2    2023-03-17 [1] CRAN (R 4.3.0)\n#&gt;    httpuv         1.6.11   2023-05-11 [1] CRAN (R 4.3.0)\n#&gt;    igraph         1.5.0    2023-06-16 [1] CRAN (R 4.3.1)\n#&gt;    inline         0.3.19   2021-05-31 [1] CRAN (R 4.3.0)\n#&gt;    jsonlite       1.8.7    2023-06-29 [1] CRAN (R 4.3.1)\n#&gt;    knitr          1.43     2023-05-25 [1] CRAN (R 4.3.0)\n#&gt;    labeling       0.4.2    2020-10-20 [1] CRAN (R 4.3.0)\n#&gt;    later          1.3.1    2023-05-02 [1] CRAN (R 4.3.0)\n#&gt;    lattice        0.21-8   2023-04-05 [2] CRAN (R 4.3.0)\n#&gt;    lazyeval     * 0.2.2    2019-03-15 [1] CRAN (R 4.3.0)\n#&gt;    lifecycle      1.0.3    2022-10-07 [1] CRAN (R 4.3.0)\n#&gt;    lme4           1.1-34   2023-07-04 [1] CRAN (R 4.3.1)\n#&gt;    loo            2.6.0    2023-03-31 [1] CRAN (R 4.3.0)\n#&gt;    magrittr       2.0.3    2022-03-30 [1] CRAN (R 4.3.0)\n#&gt;    markdown       1.7      2023-05-16 [1] CRAN (R 4.3.0)\n#&gt;    MASS           7.3-60   2023-05-04 [1] CRAN (R 4.3.0)\n#&gt;    Matrix         1.6-0    2023-07-08 [1] CRAN (R 4.3.1)\n#&gt;    matrixStats    1.0.0    2023-06-02 [1] CRAN (R 4.3.0)\n#&gt;    mgcv           1.9-0    2023-07-11 [1] CRAN (R 4.3.0)\n#&gt;    mime           0.12     2021-09-28 [1] CRAN (R 4.3.0)\n#&gt;    miniUI         0.1.1.1  2018-05-18 [1] CRAN (R 4.3.0)\n#&gt;    minqa          1.2.5    2022-10-19 [1] CRAN (R 4.3.0)\n#&gt;    munsell        0.5.0    2018-06-12 [1] CRAN (R 4.3.0)\n#&gt;    nlme           3.1-162  2023-01-31 [2] CRAN (R 4.3.0)\n#&gt;    nloptr         2.0.3    2022-05-26 [1] CRAN (R 4.3.0)\n#&gt;    pillar         1.9.0    2023-03-22 [1] CRAN (R 4.3.0)\n#&gt;    pkgbuild       1.4.2    2023-06-26 [1] CRAN (R 4.3.1)\n#&gt;    pkgconfig      2.0.3    2019-09-22 [1] CRAN (R 4.3.0)\n#&gt;    plyr           1.8.8    2022-11-11 [1] CRAN (R 4.3.0)\n#&gt;    prettyunits    1.1.1    2020-01-24 [1] CRAN (R 4.3.0)\n#&gt;    processx       3.8.2    2023-06-30 [1] CRAN (R 4.3.1)\n#&gt;    promises       1.2.0.1  2021-02-11 [1] CRAN (R 4.3.0)\n#&gt;    ps             1.7.5    2023-04-18 [1] CRAN (R 4.3.0)\n#&gt;    R6             2.5.1    2021-08-19 [1] CRAN (R 4.3.0)\n#&gt;    ragg           1.2.5    2023-01-12 [1] CRAN (R 4.3.0)\n#&gt;    Rcpp         * 1.0.11   2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;  D RcppParallel   5.1.7    2023-02-27 [1] CRAN (R 4.3.0)\n#&gt;    reshape2       1.4.4    2020-04-09 [1] CRAN (R 4.3.0)\n#&gt;    rlang          1.1.1    2023-04-28 [1] CRAN (R 4.3.0)\n#&gt;    rmarkdown      2.23     2023-07-01 [1] CRAN (R 4.3.0)\n#&gt;    rstan          2.26.22  2023-05-02 [1] local\n#&gt;    rstanarm     * 2.21.4   2023-04-08 [1] CRAN (R 4.3.0)\n#&gt;    rstantools     2.3.1    2023-03-30 [1] CRAN (R 4.3.0)\n#&gt;    rstudioapi     0.15.0   2023-07-07 [1] CRAN (R 4.3.1)\n#&gt;    scales         1.2.1    2022-08-20 [1] CRAN (R 4.3.0)\n#&gt;    sessioninfo    1.2.2    2021-12-06 [1] CRAN (R 4.3.0)\n#&gt;    shiny          1.7.4.1  2023-07-06 [1] CRAN (R 4.3.1)\n#&gt;    shinyjs        2.1.0    2021-12-23 [1] CRAN (R 4.3.0)\n#&gt;    shinystan      2.6.0    2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;    shinythemes    1.2.0    2021-01-25 [1] CRAN (R 4.3.0)\n#&gt;    StanHeaders    2.26.27  2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;    stringi        1.7.12   2023-01-11 [1] CRAN (R 4.3.0)\n#&gt;    stringr        1.5.0    2022-12-02 [1] CRAN (R 4.3.0)\n#&gt;    survival       3.5-5    2023-03-12 [2] CRAN (R 4.3.0)\n#&gt;    systemfonts    1.0.4    2022-02-11 [1] CRAN (R 4.3.0)\n#&gt;    textshaping    0.3.6    2021-10-13 [1] CRAN (R 4.3.0)\n#&gt;    threejs        0.3.3    2020-01-21 [1] CRAN (R 4.3.0)\n#&gt;    tibble         3.2.1    2023-03-20 [1] CRAN (R 4.3.0)\n#&gt;    tidyselect     1.2.0    2022-10-10 [1] CRAN (R 4.3.0)\n#&gt;    utf8           1.2.3    2023-01-31 [1] CRAN (R 4.3.0)\n#&gt;    V8             4.3.2    2023-06-30 [1] CRAN (R 4.3.1)\n#&gt;    vctrs          0.6.3    2023-06-14 [1] CRAN (R 4.3.1)\n#&gt;    withr          2.5.0    2022-03-03 [1] CRAN (R 4.3.0)\n#&gt;    xfun           0.39     2023-04-20 [1] CRAN (R 4.3.0)\n#&gt;    xtable         1.8-4    2019-04-21 [1] CRAN (R 4.3.0)\n#&gt;    xts            0.13.1   2023-04-16 [1] CRAN (R 4.3.0)\n#&gt;    yaml           2.3.7    2023-01-23 [1] CRAN (R 4.3.0)\n#&gt;    zoo            1.8-12   2023-04-13 [1] CRAN (R 4.3.0)\n#&gt; \n#&gt;  [1] C:/Users/Tristan/AppData/Local/R/win-library/4.3\n#&gt;  [2] C:/Program Files/R/R-4.3.0/library\n#&gt; \n#&gt;  D ‚îÄ‚îÄ DLL MD5 mismatch, broken installation.\n#&gt; \n#&gt; ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
  }
]